module attributes {llvm.data_layout = ""} {
  llvm.func @forward_kernel_1(%arg0: !llvm.ptr, %arg1: !llvm.ptr, %arg2: !llvm.ptr, %arg3: !llvm.ptr, %arg4: !llvm.ptr, %arg5: !llvm.ptr) attributes {Kernel, forward_kernel_1} {
    %0 = llvm.mlir.constant(1.000000e-05 : f64) : f64
    %1 = llvm.mlir.constant(16 : index) : i64
    %2 = llvm.mlir.constant(56 : index) : i64
    %3 = llvm.mlir.constant(5.000000e-01 : f32) : f32
    %4 = llvm.mlir.constant(1 : i32) : i32
    %5 = llvm.mlir.constant(1597463007 : i32) : i32
    %6 = llvm.mlir.constant(1.500000e+00 : f32) : f32
    %7 = llvm.mlir.constant(2 : index) : i64
    %8 = llvm.mlir.constant(3 : index) : i64
    %9 = llvm.mlir.constant(4 : index) : i64
    %10 = llvm.mlir.constant(5 : index) : i64
    %11 = llvm.mlir.constant(6 : index) : i64
    %12 = llvm.mlir.constant(7 : index) : i64
    %13 = llvm.mlir.constant(8 : index) : i64
    %14 = llvm.mlir.constant(9 : index) : i64
    %15 = llvm.mlir.constant(10 : index) : i64
    %16 = llvm.mlir.constant(11 : index) : i64
    %17 = llvm.mlir.constant(12 : index) : i64
    %18 = llvm.mlir.constant(13 : index) : i64
    %19 = llvm.mlir.constant(14 : index) : i64
    %20 = llvm.mlir.constant(15 : index) : i64
    %21 = llvm.mlir.constant(17 : index) : i64
    %22 = llvm.mlir.constant(18 : index) : i64
    %23 = llvm.mlir.constant(19 : index) : i64
    %24 = llvm.mlir.constant(20 : index) : i64
    %25 = llvm.mlir.constant(21 : index) : i64
    %26 = llvm.mlir.constant(22 : index) : i64
    %27 = llvm.mlir.constant(23 : index) : i64
    %28 = llvm.mlir.constant(24 : index) : i64
    %29 = llvm.mlir.constant(25 : index) : i64
    %30 = llvm.mlir.constant(26 : index) : i64
    %31 = llvm.mlir.constant(27 : index) : i64
    %32 = llvm.mlir.constant(28 : index) : i64
    %33 = llvm.mlir.constant(29 : index) : i64
    %34 = llvm.mlir.constant(30 : index) : i64
    %35 = llvm.mlir.constant(31 : index) : i64
    %36 = llvm.mlir.constant(32 : index) : i64
    %37 = llvm.mlir.constant(33 : index) : i64
    %38 = llvm.mlir.constant(34 : index) : i64
    %39 = llvm.mlir.constant(35 : index) : i64
    %40 = llvm.mlir.constant(36 : index) : i64
    %41 = llvm.mlir.constant(37 : index) : i64
    %42 = llvm.mlir.constant(38 : index) : i64
    %43 = llvm.mlir.constant(39 : index) : i64
    %44 = llvm.mlir.constant(40 : index) : i64
    %45 = llvm.mlir.constant(41 : index) : i64
    %46 = llvm.mlir.constant(42 : index) : i64
    %47 = llvm.mlir.constant(43 : index) : i64
    %48 = llvm.mlir.constant(44 : index) : i64
    %49 = llvm.mlir.constant(45 : index) : i64
    %50 = llvm.mlir.constant(46 : index) : i64
    %51 = llvm.mlir.constant(47 : index) : i64
    %52 = llvm.mlir.constant(48 : index) : i64
    %53 = llvm.mlir.constant(49 : index) : i64
    %54 = llvm.mlir.constant(50 : index) : i64
    %55 = llvm.mlir.constant(51 : index) : i64
    %56 = llvm.mlir.constant(52 : index) : i64
    %57 = llvm.mlir.constant(53 : index) : i64
    %58 = llvm.mlir.constant(54 : index) : i64
    %59 = llvm.mlir.constant(55 : index) : i64
    %60 = llvm.mlir.constant(112 : index) : i64
    %61 = llvm.mlir.constant(12544 : index) : i64
    %62 = llvm.mlir.constant(802816 : index) : i64
    %63 = llvm.mlir.constant(1 : index) : i64
    %64 = llvm.mlir.constant(0 : index) : i64
    llvm.br ^bb1
  ^bb1:  // pred: ^bb0
    llvm.br ^bb2(%64 : i64)
  ^bb2(%65: i64):  // 2 preds: ^bb1, ^bb6
    %66 = llvm.icmp "slt" %65, %1 : i64
    llvm.cond_br %66, ^bb3, ^bb7
  ^bb3:  // pred: ^bb2
    %67 = llvm.load %arg0 : !llvm.ptr -> f32
    %68 = llvm.load %arg1 : !llvm.ptr -> f32
    %69 = llvm.load %arg2 : !llvm.ptr -> f32
    %70 = llvm.load %arg3 : !llvm.ptr -> f32
    llvm.br ^bb4(%64 : i64)
  ^bb4(%71: i64):  // 2 preds: ^bb3, ^bb5
    %72 = llvm.icmp "slt" %71, %60 : i64
    llvm.cond_br %72, ^bb5, ^bb6
  ^bb5:  // pred: ^bb4
    %73 = llvm.mul %64, %62  : i64
    %74 = llvm.mul %64, %61  : i64
    %75 = llvm.add %73, %74  : i64
    %76 = llvm.mul %65, %60  : i64
    %77 = llvm.add %75, %76  : i64
    %78 = llvm.add %77, %71  : i64
    %79 = llvm.getelementptr %arg4[%78] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %80 = llvm.load %79 : !llvm.ptr -> f32
    %81 = llvm.fptrunc %0 : f64 to f32
    %82 = llvm.fadd %70, %81  : f32
    %83 = llvm.fmul %82, %3  : f32
    %84 = llvm.bitcast %82 : f32 to i32
    %85 = llvm.lshr %84, %4  : i32
    %86 = llvm.sub %5, %85  : i32
    %87 = llvm.bitcast %86 : i32 to f32
    %88 = llvm.fmul %87, %87  : f32
    %89 = llvm.fmul %88, %83  : f32
    %90 = llvm.fsub %6, %89  : f32
    %91 = llvm.fmul %90, %88  : f32
    %92 = llvm.fsub %80, %69  : f32
    %93 = llvm.fmul %92, %91  : f32
    %94 = llvm.fmul %93, %67  : f32
    %95 = llvm.fadd %94, %68  : f32
    %96 = llvm.mul %64, %62  : i64
    %97 = llvm.mul %64, %61  : i64
    %98 = llvm.add %96, %97  : i64
    %99 = llvm.mul %65, %60  : i64
    %100 = llvm.add %98, %99  : i64
    %101 = llvm.add %100, %71  : i64
    %102 = llvm.getelementptr %arg5[%101] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %95, %102 : f32, !llvm.ptr
    %103 = llvm.add %71, %63  : i64
    %104 = llvm.mul %64, %62  : i64
    %105 = llvm.mul %64, %61  : i64
    %106 = llvm.add %104, %105  : i64
    %107 = llvm.mul %65, %60  : i64
    %108 = llvm.add %106, %107  : i64
    %109 = llvm.add %108, %103  : i64
    %110 = llvm.getelementptr %arg4[%109] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %111 = llvm.load %110 : !llvm.ptr -> f32
    %112 = llvm.fptrunc %0 : f64 to f32
    %113 = llvm.fadd %70, %112  : f32
    %114 = llvm.fmul %113, %3  : f32
    %115 = llvm.bitcast %113 : f32 to i32
    %116 = llvm.lshr %115, %4  : i32
    %117 = llvm.sub %5, %116  : i32
    %118 = llvm.bitcast %117 : i32 to f32
    %119 = llvm.fmul %118, %118  : f32
    %120 = llvm.fmul %119, %114  : f32
    %121 = llvm.fsub %6, %120  : f32
    %122 = llvm.fmul %121, %119  : f32
    %123 = llvm.fsub %111, %69  : f32
    %124 = llvm.fmul %123, %122  : f32
    %125 = llvm.fmul %124, %67  : f32
    %126 = llvm.fadd %125, %68  : f32
    %127 = llvm.mul %64, %62  : i64
    %128 = llvm.mul %64, %61  : i64
    %129 = llvm.add %127, %128  : i64
    %130 = llvm.mul %65, %60  : i64
    %131 = llvm.add %129, %130  : i64
    %132 = llvm.add %131, %103  : i64
    %133 = llvm.getelementptr %arg5[%132] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %126, %133 : f32, !llvm.ptr
    %134 = llvm.add %71, %7  : i64
    %135 = llvm.mul %64, %62  : i64
    %136 = llvm.mul %64, %61  : i64
    %137 = llvm.add %135, %136  : i64
    %138 = llvm.mul %65, %60  : i64
    %139 = llvm.add %137, %138  : i64
    %140 = llvm.add %139, %134  : i64
    %141 = llvm.getelementptr %arg4[%140] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %142 = llvm.load %141 : !llvm.ptr -> f32
    %143 = llvm.fptrunc %0 : f64 to f32
    %144 = llvm.fadd %70, %143  : f32
    %145 = llvm.fmul %144, %3  : f32
    %146 = llvm.bitcast %144 : f32 to i32
    %147 = llvm.lshr %146, %4  : i32
    %148 = llvm.sub %5, %147  : i32
    %149 = llvm.bitcast %148 : i32 to f32
    %150 = llvm.fmul %149, %149  : f32
    %151 = llvm.fmul %150, %145  : f32
    %152 = llvm.fsub %6, %151  : f32
    %153 = llvm.fmul %152, %150  : f32
    %154 = llvm.fsub %142, %69  : f32
    %155 = llvm.fmul %154, %153  : f32
    %156 = llvm.fmul %155, %67  : f32
    %157 = llvm.fadd %156, %68  : f32
    %158 = llvm.mul %64, %62  : i64
    %159 = llvm.mul %64, %61  : i64
    %160 = llvm.add %158, %159  : i64
    %161 = llvm.mul %65, %60  : i64
    %162 = llvm.add %160, %161  : i64
    %163 = llvm.add %162, %134  : i64
    %164 = llvm.getelementptr %arg5[%163] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %157, %164 : f32, !llvm.ptr
    %165 = llvm.add %71, %8  : i64
    %166 = llvm.mul %64, %62  : i64
    %167 = llvm.mul %64, %61  : i64
    %168 = llvm.add %166, %167  : i64
    %169 = llvm.mul %65, %60  : i64
    %170 = llvm.add %168, %169  : i64
    %171 = llvm.add %170, %165  : i64
    %172 = llvm.getelementptr %arg4[%171] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %173 = llvm.load %172 : !llvm.ptr -> f32
    %174 = llvm.fptrunc %0 : f64 to f32
    %175 = llvm.fadd %70, %174  : f32
    %176 = llvm.fmul %175, %3  : f32
    %177 = llvm.bitcast %175 : f32 to i32
    %178 = llvm.lshr %177, %4  : i32
    %179 = llvm.sub %5, %178  : i32
    %180 = llvm.bitcast %179 : i32 to f32
    %181 = llvm.fmul %180, %180  : f32
    %182 = llvm.fmul %181, %176  : f32
    %183 = llvm.fsub %6, %182  : f32
    %184 = llvm.fmul %183, %181  : f32
    %185 = llvm.fsub %173, %69  : f32
    %186 = llvm.fmul %185, %184  : f32
    %187 = llvm.fmul %186, %67  : f32
    %188 = llvm.fadd %187, %68  : f32
    %189 = llvm.mul %64, %62  : i64
    %190 = llvm.mul %64, %61  : i64
    %191 = llvm.add %189, %190  : i64
    %192 = llvm.mul %65, %60  : i64
    %193 = llvm.add %191, %192  : i64
    %194 = llvm.add %193, %165  : i64
    %195 = llvm.getelementptr %arg5[%194] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %188, %195 : f32, !llvm.ptr
    %196 = llvm.add %71, %9  : i64
    %197 = llvm.mul %64, %62  : i64
    %198 = llvm.mul %64, %61  : i64
    %199 = llvm.add %197, %198  : i64
    %200 = llvm.mul %65, %60  : i64
    %201 = llvm.add %199, %200  : i64
    %202 = llvm.add %201, %196  : i64
    %203 = llvm.getelementptr %arg4[%202] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %204 = llvm.load %203 : !llvm.ptr -> f32
    %205 = llvm.fptrunc %0 : f64 to f32
    %206 = llvm.fadd %70, %205  : f32
    %207 = llvm.fmul %206, %3  : f32
    %208 = llvm.bitcast %206 : f32 to i32
    %209 = llvm.lshr %208, %4  : i32
    %210 = llvm.sub %5, %209  : i32
    %211 = llvm.bitcast %210 : i32 to f32
    %212 = llvm.fmul %211, %211  : f32
    %213 = llvm.fmul %212, %207  : f32
    %214 = llvm.fsub %6, %213  : f32
    %215 = llvm.fmul %214, %212  : f32
    %216 = llvm.fsub %204, %69  : f32
    %217 = llvm.fmul %216, %215  : f32
    %218 = llvm.fmul %217, %67  : f32
    %219 = llvm.fadd %218, %68  : f32
    %220 = llvm.mul %64, %62  : i64
    %221 = llvm.mul %64, %61  : i64
    %222 = llvm.add %220, %221  : i64
    %223 = llvm.mul %65, %60  : i64
    %224 = llvm.add %222, %223  : i64
    %225 = llvm.add %224, %196  : i64
    %226 = llvm.getelementptr %arg5[%225] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %219, %226 : f32, !llvm.ptr
    %227 = llvm.add %71, %10  : i64
    %228 = llvm.mul %64, %62  : i64
    %229 = llvm.mul %64, %61  : i64
    %230 = llvm.add %228, %229  : i64
    %231 = llvm.mul %65, %60  : i64
    %232 = llvm.add %230, %231  : i64
    %233 = llvm.add %232, %227  : i64
    %234 = llvm.getelementptr %arg4[%233] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %235 = llvm.load %234 : !llvm.ptr -> f32
    %236 = llvm.fptrunc %0 : f64 to f32
    %237 = llvm.fadd %70, %236  : f32
    %238 = llvm.fmul %237, %3  : f32
    %239 = llvm.bitcast %237 : f32 to i32
    %240 = llvm.lshr %239, %4  : i32
    %241 = llvm.sub %5, %240  : i32
    %242 = llvm.bitcast %241 : i32 to f32
    %243 = llvm.fmul %242, %242  : f32
    %244 = llvm.fmul %243, %238  : f32
    %245 = llvm.fsub %6, %244  : f32
    %246 = llvm.fmul %245, %243  : f32
    %247 = llvm.fsub %235, %69  : f32
    %248 = llvm.fmul %247, %246  : f32
    %249 = llvm.fmul %248, %67  : f32
    %250 = llvm.fadd %249, %68  : f32
    %251 = llvm.mul %64, %62  : i64
    %252 = llvm.mul %64, %61  : i64
    %253 = llvm.add %251, %252  : i64
    %254 = llvm.mul %65, %60  : i64
    %255 = llvm.add %253, %254  : i64
    %256 = llvm.add %255, %227  : i64
    %257 = llvm.getelementptr %arg5[%256] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %250, %257 : f32, !llvm.ptr
    %258 = llvm.add %71, %11  : i64
    %259 = llvm.mul %64, %62  : i64
    %260 = llvm.mul %64, %61  : i64
    %261 = llvm.add %259, %260  : i64
    %262 = llvm.mul %65, %60  : i64
    %263 = llvm.add %261, %262  : i64
    %264 = llvm.add %263, %258  : i64
    %265 = llvm.getelementptr %arg4[%264] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %266 = llvm.load %265 : !llvm.ptr -> f32
    %267 = llvm.fptrunc %0 : f64 to f32
    %268 = llvm.fadd %70, %267  : f32
    %269 = llvm.fmul %268, %3  : f32
    %270 = llvm.bitcast %268 : f32 to i32
    %271 = llvm.lshr %270, %4  : i32
    %272 = llvm.sub %5, %271  : i32
    %273 = llvm.bitcast %272 : i32 to f32
    %274 = llvm.fmul %273, %273  : f32
    %275 = llvm.fmul %274, %269  : f32
    %276 = llvm.fsub %6, %275  : f32
    %277 = llvm.fmul %276, %274  : f32
    %278 = llvm.fsub %266, %69  : f32
    %279 = llvm.fmul %278, %277  : f32
    %280 = llvm.fmul %279, %67  : f32
    %281 = llvm.fadd %280, %68  : f32
    %282 = llvm.mul %64, %62  : i64
    %283 = llvm.mul %64, %61  : i64
    %284 = llvm.add %282, %283  : i64
    %285 = llvm.mul %65, %60  : i64
    %286 = llvm.add %284, %285  : i64
    %287 = llvm.add %286, %258  : i64
    %288 = llvm.getelementptr %arg5[%287] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %281, %288 : f32, !llvm.ptr
    %289 = llvm.add %71, %12  : i64
    %290 = llvm.mul %64, %62  : i64
    %291 = llvm.mul %64, %61  : i64
    %292 = llvm.add %290, %291  : i64
    %293 = llvm.mul %65, %60  : i64
    %294 = llvm.add %292, %293  : i64
    %295 = llvm.add %294, %289  : i64
    %296 = llvm.getelementptr %arg4[%295] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %297 = llvm.load %296 : !llvm.ptr -> f32
    %298 = llvm.fptrunc %0 : f64 to f32
    %299 = llvm.fadd %70, %298  : f32
    %300 = llvm.fmul %299, %3  : f32
    %301 = llvm.bitcast %299 : f32 to i32
    %302 = llvm.lshr %301, %4  : i32
    %303 = llvm.sub %5, %302  : i32
    %304 = llvm.bitcast %303 : i32 to f32
    %305 = llvm.fmul %304, %304  : f32
    %306 = llvm.fmul %305, %300  : f32
    %307 = llvm.fsub %6, %306  : f32
    %308 = llvm.fmul %307, %305  : f32
    %309 = llvm.fsub %297, %69  : f32
    %310 = llvm.fmul %309, %308  : f32
    %311 = llvm.fmul %310, %67  : f32
    %312 = llvm.fadd %311, %68  : f32
    %313 = llvm.mul %64, %62  : i64
    %314 = llvm.mul %64, %61  : i64
    %315 = llvm.add %313, %314  : i64
    %316 = llvm.mul %65, %60  : i64
    %317 = llvm.add %315, %316  : i64
    %318 = llvm.add %317, %289  : i64
    %319 = llvm.getelementptr %arg5[%318] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %312, %319 : f32, !llvm.ptr
    %320 = llvm.add %71, %13  : i64
    %321 = llvm.mul %64, %62  : i64
    %322 = llvm.mul %64, %61  : i64
    %323 = llvm.add %321, %322  : i64
    %324 = llvm.mul %65, %60  : i64
    %325 = llvm.add %323, %324  : i64
    %326 = llvm.add %325, %320  : i64
    %327 = llvm.getelementptr %arg4[%326] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %328 = llvm.load %327 : !llvm.ptr -> f32
    %329 = llvm.fptrunc %0 : f64 to f32
    %330 = llvm.fadd %70, %329  : f32
    %331 = llvm.fmul %330, %3  : f32
    %332 = llvm.bitcast %330 : f32 to i32
    %333 = llvm.lshr %332, %4  : i32
    %334 = llvm.sub %5, %333  : i32
    %335 = llvm.bitcast %334 : i32 to f32
    %336 = llvm.fmul %335, %335  : f32
    %337 = llvm.fmul %336, %331  : f32
    %338 = llvm.fsub %6, %337  : f32
    %339 = llvm.fmul %338, %336  : f32
    %340 = llvm.fsub %328, %69  : f32
    %341 = llvm.fmul %340, %339  : f32
    %342 = llvm.fmul %341, %67  : f32
    %343 = llvm.fadd %342, %68  : f32
    %344 = llvm.mul %64, %62  : i64
    %345 = llvm.mul %64, %61  : i64
    %346 = llvm.add %344, %345  : i64
    %347 = llvm.mul %65, %60  : i64
    %348 = llvm.add %346, %347  : i64
    %349 = llvm.add %348, %320  : i64
    %350 = llvm.getelementptr %arg5[%349] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %343, %350 : f32, !llvm.ptr
    %351 = llvm.add %71, %14  : i64
    %352 = llvm.mul %64, %62  : i64
    %353 = llvm.mul %64, %61  : i64
    %354 = llvm.add %352, %353  : i64
    %355 = llvm.mul %65, %60  : i64
    %356 = llvm.add %354, %355  : i64
    %357 = llvm.add %356, %351  : i64
    %358 = llvm.getelementptr %arg4[%357] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %359 = llvm.load %358 : !llvm.ptr -> f32
    %360 = llvm.fptrunc %0 : f64 to f32
    %361 = llvm.fadd %70, %360  : f32
    %362 = llvm.fmul %361, %3  : f32
    %363 = llvm.bitcast %361 : f32 to i32
    %364 = llvm.lshr %363, %4  : i32
    %365 = llvm.sub %5, %364  : i32
    %366 = llvm.bitcast %365 : i32 to f32
    %367 = llvm.fmul %366, %366  : f32
    %368 = llvm.fmul %367, %362  : f32
    %369 = llvm.fsub %6, %368  : f32
    %370 = llvm.fmul %369, %367  : f32
    %371 = llvm.fsub %359, %69  : f32
    %372 = llvm.fmul %371, %370  : f32
    %373 = llvm.fmul %372, %67  : f32
    %374 = llvm.fadd %373, %68  : f32
    %375 = llvm.mul %64, %62  : i64
    %376 = llvm.mul %64, %61  : i64
    %377 = llvm.add %375, %376  : i64
    %378 = llvm.mul %65, %60  : i64
    %379 = llvm.add %377, %378  : i64
    %380 = llvm.add %379, %351  : i64
    %381 = llvm.getelementptr %arg5[%380] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %374, %381 : f32, !llvm.ptr
    %382 = llvm.add %71, %15  : i64
    %383 = llvm.mul %64, %62  : i64
    %384 = llvm.mul %64, %61  : i64
    %385 = llvm.add %383, %384  : i64
    %386 = llvm.mul %65, %60  : i64
    %387 = llvm.add %385, %386  : i64
    %388 = llvm.add %387, %382  : i64
    %389 = llvm.getelementptr %arg4[%388] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %390 = llvm.load %389 : !llvm.ptr -> f32
    %391 = llvm.fptrunc %0 : f64 to f32
    %392 = llvm.fadd %70, %391  : f32
    %393 = llvm.fmul %392, %3  : f32
    %394 = llvm.bitcast %392 : f32 to i32
    %395 = llvm.lshr %394, %4  : i32
    %396 = llvm.sub %5, %395  : i32
    %397 = llvm.bitcast %396 : i32 to f32
    %398 = llvm.fmul %397, %397  : f32
    %399 = llvm.fmul %398, %393  : f32
    %400 = llvm.fsub %6, %399  : f32
    %401 = llvm.fmul %400, %398  : f32
    %402 = llvm.fsub %390, %69  : f32
    %403 = llvm.fmul %402, %401  : f32
    %404 = llvm.fmul %403, %67  : f32
    %405 = llvm.fadd %404, %68  : f32
    %406 = llvm.mul %64, %62  : i64
    %407 = llvm.mul %64, %61  : i64
    %408 = llvm.add %406, %407  : i64
    %409 = llvm.mul %65, %60  : i64
    %410 = llvm.add %408, %409  : i64
    %411 = llvm.add %410, %382  : i64
    %412 = llvm.getelementptr %arg5[%411] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %405, %412 : f32, !llvm.ptr
    %413 = llvm.add %71, %16  : i64
    %414 = llvm.mul %64, %62  : i64
    %415 = llvm.mul %64, %61  : i64
    %416 = llvm.add %414, %415  : i64
    %417 = llvm.mul %65, %60  : i64
    %418 = llvm.add %416, %417  : i64
    %419 = llvm.add %418, %413  : i64
    %420 = llvm.getelementptr %arg4[%419] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %421 = llvm.load %420 : !llvm.ptr -> f32
    %422 = llvm.fptrunc %0 : f64 to f32
    %423 = llvm.fadd %70, %422  : f32
    %424 = llvm.fmul %423, %3  : f32
    %425 = llvm.bitcast %423 : f32 to i32
    %426 = llvm.lshr %425, %4  : i32
    %427 = llvm.sub %5, %426  : i32
    %428 = llvm.bitcast %427 : i32 to f32
    %429 = llvm.fmul %428, %428  : f32
    %430 = llvm.fmul %429, %424  : f32
    %431 = llvm.fsub %6, %430  : f32
    %432 = llvm.fmul %431, %429  : f32
    %433 = llvm.fsub %421, %69  : f32
    %434 = llvm.fmul %433, %432  : f32
    %435 = llvm.fmul %434, %67  : f32
    %436 = llvm.fadd %435, %68  : f32
    %437 = llvm.mul %64, %62  : i64
    %438 = llvm.mul %64, %61  : i64
    %439 = llvm.add %437, %438  : i64
    %440 = llvm.mul %65, %60  : i64
    %441 = llvm.add %439, %440  : i64
    %442 = llvm.add %441, %413  : i64
    %443 = llvm.getelementptr %arg5[%442] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %436, %443 : f32, !llvm.ptr
    %444 = llvm.add %71, %17  : i64
    %445 = llvm.mul %64, %62  : i64
    %446 = llvm.mul %64, %61  : i64
    %447 = llvm.add %445, %446  : i64
    %448 = llvm.mul %65, %60  : i64
    %449 = llvm.add %447, %448  : i64
    %450 = llvm.add %449, %444  : i64
    %451 = llvm.getelementptr %arg4[%450] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %452 = llvm.load %451 : !llvm.ptr -> f32
    %453 = llvm.fptrunc %0 : f64 to f32
    %454 = llvm.fadd %70, %453  : f32
    %455 = llvm.fmul %454, %3  : f32
    %456 = llvm.bitcast %454 : f32 to i32
    %457 = llvm.lshr %456, %4  : i32
    %458 = llvm.sub %5, %457  : i32
    %459 = llvm.bitcast %458 : i32 to f32
    %460 = llvm.fmul %459, %459  : f32
    %461 = llvm.fmul %460, %455  : f32
    %462 = llvm.fsub %6, %461  : f32
    %463 = llvm.fmul %462, %460  : f32
    %464 = llvm.fsub %452, %69  : f32
    %465 = llvm.fmul %464, %463  : f32
    %466 = llvm.fmul %465, %67  : f32
    %467 = llvm.fadd %466, %68  : f32
    %468 = llvm.mul %64, %62  : i64
    %469 = llvm.mul %64, %61  : i64
    %470 = llvm.add %468, %469  : i64
    %471 = llvm.mul %65, %60  : i64
    %472 = llvm.add %470, %471  : i64
    %473 = llvm.add %472, %444  : i64
    %474 = llvm.getelementptr %arg5[%473] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %467, %474 : f32, !llvm.ptr
    %475 = llvm.add %71, %18  : i64
    %476 = llvm.mul %64, %62  : i64
    %477 = llvm.mul %64, %61  : i64
    %478 = llvm.add %476, %477  : i64
    %479 = llvm.mul %65, %60  : i64
    %480 = llvm.add %478, %479  : i64
    %481 = llvm.add %480, %475  : i64
    %482 = llvm.getelementptr %arg4[%481] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %483 = llvm.load %482 : !llvm.ptr -> f32
    %484 = llvm.fptrunc %0 : f64 to f32
    %485 = llvm.fadd %70, %484  : f32
    %486 = llvm.fmul %485, %3  : f32
    %487 = llvm.bitcast %485 : f32 to i32
    %488 = llvm.lshr %487, %4  : i32
    %489 = llvm.sub %5, %488  : i32
    %490 = llvm.bitcast %489 : i32 to f32
    %491 = llvm.fmul %490, %490  : f32
    %492 = llvm.fmul %491, %486  : f32
    %493 = llvm.fsub %6, %492  : f32
    %494 = llvm.fmul %493, %491  : f32
    %495 = llvm.fsub %483, %69  : f32
    %496 = llvm.fmul %495, %494  : f32
    %497 = llvm.fmul %496, %67  : f32
    %498 = llvm.fadd %497, %68  : f32
    %499 = llvm.mul %64, %62  : i64
    %500 = llvm.mul %64, %61  : i64
    %501 = llvm.add %499, %500  : i64
    %502 = llvm.mul %65, %60  : i64
    %503 = llvm.add %501, %502  : i64
    %504 = llvm.add %503, %475  : i64
    %505 = llvm.getelementptr %arg5[%504] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %498, %505 : f32, !llvm.ptr
    %506 = llvm.add %71, %19  : i64
    %507 = llvm.mul %64, %62  : i64
    %508 = llvm.mul %64, %61  : i64
    %509 = llvm.add %507, %508  : i64
    %510 = llvm.mul %65, %60  : i64
    %511 = llvm.add %509, %510  : i64
    %512 = llvm.add %511, %506  : i64
    %513 = llvm.getelementptr %arg4[%512] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %514 = llvm.load %513 : !llvm.ptr -> f32
    %515 = llvm.fptrunc %0 : f64 to f32
    %516 = llvm.fadd %70, %515  : f32
    %517 = llvm.fmul %516, %3  : f32
    %518 = llvm.bitcast %516 : f32 to i32
    %519 = llvm.lshr %518, %4  : i32
    %520 = llvm.sub %5, %519  : i32
    %521 = llvm.bitcast %520 : i32 to f32
    %522 = llvm.fmul %521, %521  : f32
    %523 = llvm.fmul %522, %517  : f32
    %524 = llvm.fsub %6, %523  : f32
    %525 = llvm.fmul %524, %522  : f32
    %526 = llvm.fsub %514, %69  : f32
    %527 = llvm.fmul %526, %525  : f32
    %528 = llvm.fmul %527, %67  : f32
    %529 = llvm.fadd %528, %68  : f32
    %530 = llvm.mul %64, %62  : i64
    %531 = llvm.mul %64, %61  : i64
    %532 = llvm.add %530, %531  : i64
    %533 = llvm.mul %65, %60  : i64
    %534 = llvm.add %532, %533  : i64
    %535 = llvm.add %534, %506  : i64
    %536 = llvm.getelementptr %arg5[%535] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %529, %536 : f32, !llvm.ptr
    %537 = llvm.add %71, %20  : i64
    %538 = llvm.mul %64, %62  : i64
    %539 = llvm.mul %64, %61  : i64
    %540 = llvm.add %538, %539  : i64
    %541 = llvm.mul %65, %60  : i64
    %542 = llvm.add %540, %541  : i64
    %543 = llvm.add %542, %537  : i64
    %544 = llvm.getelementptr %arg4[%543] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %545 = llvm.load %544 : !llvm.ptr -> f32
    %546 = llvm.fptrunc %0 : f64 to f32
    %547 = llvm.fadd %70, %546  : f32
    %548 = llvm.fmul %547, %3  : f32
    %549 = llvm.bitcast %547 : f32 to i32
    %550 = llvm.lshr %549, %4  : i32
    %551 = llvm.sub %5, %550  : i32
    %552 = llvm.bitcast %551 : i32 to f32
    %553 = llvm.fmul %552, %552  : f32
    %554 = llvm.fmul %553, %548  : f32
    %555 = llvm.fsub %6, %554  : f32
    %556 = llvm.fmul %555, %553  : f32
    %557 = llvm.fsub %545, %69  : f32
    %558 = llvm.fmul %557, %556  : f32
    %559 = llvm.fmul %558, %67  : f32
    %560 = llvm.fadd %559, %68  : f32
    %561 = llvm.mul %64, %62  : i64
    %562 = llvm.mul %64, %61  : i64
    %563 = llvm.add %561, %562  : i64
    %564 = llvm.mul %65, %60  : i64
    %565 = llvm.add %563, %564  : i64
    %566 = llvm.add %565, %537  : i64
    %567 = llvm.getelementptr %arg5[%566] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %560, %567 : f32, !llvm.ptr
    %568 = llvm.add %71, %1  : i64
    %569 = llvm.mul %64, %62  : i64
    %570 = llvm.mul %64, %61  : i64
    %571 = llvm.add %569, %570  : i64
    %572 = llvm.mul %65, %60  : i64
    %573 = llvm.add %571, %572  : i64
    %574 = llvm.add %573, %568  : i64
    %575 = llvm.getelementptr %arg4[%574] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %576 = llvm.load %575 : !llvm.ptr -> f32
    %577 = llvm.fptrunc %0 : f64 to f32
    %578 = llvm.fadd %70, %577  : f32
    %579 = llvm.fmul %578, %3  : f32
    %580 = llvm.bitcast %578 : f32 to i32
    %581 = llvm.lshr %580, %4  : i32
    %582 = llvm.sub %5, %581  : i32
    %583 = llvm.bitcast %582 : i32 to f32
    %584 = llvm.fmul %583, %583  : f32
    %585 = llvm.fmul %584, %579  : f32
    %586 = llvm.fsub %6, %585  : f32
    %587 = llvm.fmul %586, %584  : f32
    %588 = llvm.fsub %576, %69  : f32
    %589 = llvm.fmul %588, %587  : f32
    %590 = llvm.fmul %589, %67  : f32
    %591 = llvm.fadd %590, %68  : f32
    %592 = llvm.mul %64, %62  : i64
    %593 = llvm.mul %64, %61  : i64
    %594 = llvm.add %592, %593  : i64
    %595 = llvm.mul %65, %60  : i64
    %596 = llvm.add %594, %595  : i64
    %597 = llvm.add %596, %568  : i64
    %598 = llvm.getelementptr %arg5[%597] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %591, %598 : f32, !llvm.ptr
    %599 = llvm.add %71, %21  : i64
    %600 = llvm.mul %64, %62  : i64
    %601 = llvm.mul %64, %61  : i64
    %602 = llvm.add %600, %601  : i64
    %603 = llvm.mul %65, %60  : i64
    %604 = llvm.add %602, %603  : i64
    %605 = llvm.add %604, %599  : i64
    %606 = llvm.getelementptr %arg4[%605] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %607 = llvm.load %606 : !llvm.ptr -> f32
    %608 = llvm.fptrunc %0 : f64 to f32
    %609 = llvm.fadd %70, %608  : f32
    %610 = llvm.fmul %609, %3  : f32
    %611 = llvm.bitcast %609 : f32 to i32
    %612 = llvm.lshr %611, %4  : i32
    %613 = llvm.sub %5, %612  : i32
    %614 = llvm.bitcast %613 : i32 to f32
    %615 = llvm.fmul %614, %614  : f32
    %616 = llvm.fmul %615, %610  : f32
    %617 = llvm.fsub %6, %616  : f32
    %618 = llvm.fmul %617, %615  : f32
    %619 = llvm.fsub %607, %69  : f32
    %620 = llvm.fmul %619, %618  : f32
    %621 = llvm.fmul %620, %67  : f32
    %622 = llvm.fadd %621, %68  : f32
    %623 = llvm.mul %64, %62  : i64
    %624 = llvm.mul %64, %61  : i64
    %625 = llvm.add %623, %624  : i64
    %626 = llvm.mul %65, %60  : i64
    %627 = llvm.add %625, %626  : i64
    %628 = llvm.add %627, %599  : i64
    %629 = llvm.getelementptr %arg5[%628] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %622, %629 : f32, !llvm.ptr
    %630 = llvm.add %71, %22  : i64
    %631 = llvm.mul %64, %62  : i64
    %632 = llvm.mul %64, %61  : i64
    %633 = llvm.add %631, %632  : i64
    %634 = llvm.mul %65, %60  : i64
    %635 = llvm.add %633, %634  : i64
    %636 = llvm.add %635, %630  : i64
    %637 = llvm.getelementptr %arg4[%636] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %638 = llvm.load %637 : !llvm.ptr -> f32
    %639 = llvm.fptrunc %0 : f64 to f32
    %640 = llvm.fadd %70, %639  : f32
    %641 = llvm.fmul %640, %3  : f32
    %642 = llvm.bitcast %640 : f32 to i32
    %643 = llvm.lshr %642, %4  : i32
    %644 = llvm.sub %5, %643  : i32
    %645 = llvm.bitcast %644 : i32 to f32
    %646 = llvm.fmul %645, %645  : f32
    %647 = llvm.fmul %646, %641  : f32
    %648 = llvm.fsub %6, %647  : f32
    %649 = llvm.fmul %648, %646  : f32
    %650 = llvm.fsub %638, %69  : f32
    %651 = llvm.fmul %650, %649  : f32
    %652 = llvm.fmul %651, %67  : f32
    %653 = llvm.fadd %652, %68  : f32
    %654 = llvm.mul %64, %62  : i64
    %655 = llvm.mul %64, %61  : i64
    %656 = llvm.add %654, %655  : i64
    %657 = llvm.mul %65, %60  : i64
    %658 = llvm.add %656, %657  : i64
    %659 = llvm.add %658, %630  : i64
    %660 = llvm.getelementptr %arg5[%659] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %653, %660 : f32, !llvm.ptr
    %661 = llvm.add %71, %23  : i64
    %662 = llvm.mul %64, %62  : i64
    %663 = llvm.mul %64, %61  : i64
    %664 = llvm.add %662, %663  : i64
    %665 = llvm.mul %65, %60  : i64
    %666 = llvm.add %664, %665  : i64
    %667 = llvm.add %666, %661  : i64
    %668 = llvm.getelementptr %arg4[%667] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %669 = llvm.load %668 : !llvm.ptr -> f32
    %670 = llvm.fptrunc %0 : f64 to f32
    %671 = llvm.fadd %70, %670  : f32
    %672 = llvm.fmul %671, %3  : f32
    %673 = llvm.bitcast %671 : f32 to i32
    %674 = llvm.lshr %673, %4  : i32
    %675 = llvm.sub %5, %674  : i32
    %676 = llvm.bitcast %675 : i32 to f32
    %677 = llvm.fmul %676, %676  : f32
    %678 = llvm.fmul %677, %672  : f32
    %679 = llvm.fsub %6, %678  : f32
    %680 = llvm.fmul %679, %677  : f32
    %681 = llvm.fsub %669, %69  : f32
    %682 = llvm.fmul %681, %680  : f32
    %683 = llvm.fmul %682, %67  : f32
    %684 = llvm.fadd %683, %68  : f32
    %685 = llvm.mul %64, %62  : i64
    %686 = llvm.mul %64, %61  : i64
    %687 = llvm.add %685, %686  : i64
    %688 = llvm.mul %65, %60  : i64
    %689 = llvm.add %687, %688  : i64
    %690 = llvm.add %689, %661  : i64
    %691 = llvm.getelementptr %arg5[%690] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %684, %691 : f32, !llvm.ptr
    %692 = llvm.add %71, %24  : i64
    %693 = llvm.mul %64, %62  : i64
    %694 = llvm.mul %64, %61  : i64
    %695 = llvm.add %693, %694  : i64
    %696 = llvm.mul %65, %60  : i64
    %697 = llvm.add %695, %696  : i64
    %698 = llvm.add %697, %692  : i64
    %699 = llvm.getelementptr %arg4[%698] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %700 = llvm.load %699 : !llvm.ptr -> f32
    %701 = llvm.fptrunc %0 : f64 to f32
    %702 = llvm.fadd %70, %701  : f32
    %703 = llvm.fmul %702, %3  : f32
    %704 = llvm.bitcast %702 : f32 to i32
    %705 = llvm.lshr %704, %4  : i32
    %706 = llvm.sub %5, %705  : i32
    %707 = llvm.bitcast %706 : i32 to f32
    %708 = llvm.fmul %707, %707  : f32
    %709 = llvm.fmul %708, %703  : f32
    %710 = llvm.fsub %6, %709  : f32
    %711 = llvm.fmul %710, %708  : f32
    %712 = llvm.fsub %700, %69  : f32
    %713 = llvm.fmul %712, %711  : f32
    %714 = llvm.fmul %713, %67  : f32
    %715 = llvm.fadd %714, %68  : f32
    %716 = llvm.mul %64, %62  : i64
    %717 = llvm.mul %64, %61  : i64
    %718 = llvm.add %716, %717  : i64
    %719 = llvm.mul %65, %60  : i64
    %720 = llvm.add %718, %719  : i64
    %721 = llvm.add %720, %692  : i64
    %722 = llvm.getelementptr %arg5[%721] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %715, %722 : f32, !llvm.ptr
    %723 = llvm.add %71, %25  : i64
    %724 = llvm.mul %64, %62  : i64
    %725 = llvm.mul %64, %61  : i64
    %726 = llvm.add %724, %725  : i64
    %727 = llvm.mul %65, %60  : i64
    %728 = llvm.add %726, %727  : i64
    %729 = llvm.add %728, %723  : i64
    %730 = llvm.getelementptr %arg4[%729] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %731 = llvm.load %730 : !llvm.ptr -> f32
    %732 = llvm.fptrunc %0 : f64 to f32
    %733 = llvm.fadd %70, %732  : f32
    %734 = llvm.fmul %733, %3  : f32
    %735 = llvm.bitcast %733 : f32 to i32
    %736 = llvm.lshr %735, %4  : i32
    %737 = llvm.sub %5, %736  : i32
    %738 = llvm.bitcast %737 : i32 to f32
    %739 = llvm.fmul %738, %738  : f32
    %740 = llvm.fmul %739, %734  : f32
    %741 = llvm.fsub %6, %740  : f32
    %742 = llvm.fmul %741, %739  : f32
    %743 = llvm.fsub %731, %69  : f32
    %744 = llvm.fmul %743, %742  : f32
    %745 = llvm.fmul %744, %67  : f32
    %746 = llvm.fadd %745, %68  : f32
    %747 = llvm.mul %64, %62  : i64
    %748 = llvm.mul %64, %61  : i64
    %749 = llvm.add %747, %748  : i64
    %750 = llvm.mul %65, %60  : i64
    %751 = llvm.add %749, %750  : i64
    %752 = llvm.add %751, %723  : i64
    %753 = llvm.getelementptr %arg5[%752] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %746, %753 : f32, !llvm.ptr
    %754 = llvm.add %71, %26  : i64
    %755 = llvm.mul %64, %62  : i64
    %756 = llvm.mul %64, %61  : i64
    %757 = llvm.add %755, %756  : i64
    %758 = llvm.mul %65, %60  : i64
    %759 = llvm.add %757, %758  : i64
    %760 = llvm.add %759, %754  : i64
    %761 = llvm.getelementptr %arg4[%760] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %762 = llvm.load %761 : !llvm.ptr -> f32
    %763 = llvm.fptrunc %0 : f64 to f32
    %764 = llvm.fadd %70, %763  : f32
    %765 = llvm.fmul %764, %3  : f32
    %766 = llvm.bitcast %764 : f32 to i32
    %767 = llvm.lshr %766, %4  : i32
    %768 = llvm.sub %5, %767  : i32
    %769 = llvm.bitcast %768 : i32 to f32
    %770 = llvm.fmul %769, %769  : f32
    %771 = llvm.fmul %770, %765  : f32
    %772 = llvm.fsub %6, %771  : f32
    %773 = llvm.fmul %772, %770  : f32
    %774 = llvm.fsub %762, %69  : f32
    %775 = llvm.fmul %774, %773  : f32
    %776 = llvm.fmul %775, %67  : f32
    %777 = llvm.fadd %776, %68  : f32
    %778 = llvm.mul %64, %62  : i64
    %779 = llvm.mul %64, %61  : i64
    %780 = llvm.add %778, %779  : i64
    %781 = llvm.mul %65, %60  : i64
    %782 = llvm.add %780, %781  : i64
    %783 = llvm.add %782, %754  : i64
    %784 = llvm.getelementptr %arg5[%783] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %777, %784 : f32, !llvm.ptr
    %785 = llvm.add %71, %27  : i64
    %786 = llvm.mul %64, %62  : i64
    %787 = llvm.mul %64, %61  : i64
    %788 = llvm.add %786, %787  : i64
    %789 = llvm.mul %65, %60  : i64
    %790 = llvm.add %788, %789  : i64
    %791 = llvm.add %790, %785  : i64
    %792 = llvm.getelementptr %arg4[%791] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %793 = llvm.load %792 : !llvm.ptr -> f32
    %794 = llvm.fptrunc %0 : f64 to f32
    %795 = llvm.fadd %70, %794  : f32
    %796 = llvm.fmul %795, %3  : f32
    %797 = llvm.bitcast %795 : f32 to i32
    %798 = llvm.lshr %797, %4  : i32
    %799 = llvm.sub %5, %798  : i32
    %800 = llvm.bitcast %799 : i32 to f32
    %801 = llvm.fmul %800, %800  : f32
    %802 = llvm.fmul %801, %796  : f32
    %803 = llvm.fsub %6, %802  : f32
    %804 = llvm.fmul %803, %801  : f32
    %805 = llvm.fsub %793, %69  : f32
    %806 = llvm.fmul %805, %804  : f32
    %807 = llvm.fmul %806, %67  : f32
    %808 = llvm.fadd %807, %68  : f32
    %809 = llvm.mul %64, %62  : i64
    %810 = llvm.mul %64, %61  : i64
    %811 = llvm.add %809, %810  : i64
    %812 = llvm.mul %65, %60  : i64
    %813 = llvm.add %811, %812  : i64
    %814 = llvm.add %813, %785  : i64
    %815 = llvm.getelementptr %arg5[%814] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %808, %815 : f32, !llvm.ptr
    %816 = llvm.add %71, %28  : i64
    %817 = llvm.mul %64, %62  : i64
    %818 = llvm.mul %64, %61  : i64
    %819 = llvm.add %817, %818  : i64
    %820 = llvm.mul %65, %60  : i64
    %821 = llvm.add %819, %820  : i64
    %822 = llvm.add %821, %816  : i64
    %823 = llvm.getelementptr %arg4[%822] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %824 = llvm.load %823 : !llvm.ptr -> f32
    %825 = llvm.fptrunc %0 : f64 to f32
    %826 = llvm.fadd %70, %825  : f32
    %827 = llvm.fmul %826, %3  : f32
    %828 = llvm.bitcast %826 : f32 to i32
    %829 = llvm.lshr %828, %4  : i32
    %830 = llvm.sub %5, %829  : i32
    %831 = llvm.bitcast %830 : i32 to f32
    %832 = llvm.fmul %831, %831  : f32
    %833 = llvm.fmul %832, %827  : f32
    %834 = llvm.fsub %6, %833  : f32
    %835 = llvm.fmul %834, %832  : f32
    %836 = llvm.fsub %824, %69  : f32
    %837 = llvm.fmul %836, %835  : f32
    %838 = llvm.fmul %837, %67  : f32
    %839 = llvm.fadd %838, %68  : f32
    %840 = llvm.mul %64, %62  : i64
    %841 = llvm.mul %64, %61  : i64
    %842 = llvm.add %840, %841  : i64
    %843 = llvm.mul %65, %60  : i64
    %844 = llvm.add %842, %843  : i64
    %845 = llvm.add %844, %816  : i64
    %846 = llvm.getelementptr %arg5[%845] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %839, %846 : f32, !llvm.ptr
    %847 = llvm.add %71, %29  : i64
    %848 = llvm.mul %64, %62  : i64
    %849 = llvm.mul %64, %61  : i64
    %850 = llvm.add %848, %849  : i64
    %851 = llvm.mul %65, %60  : i64
    %852 = llvm.add %850, %851  : i64
    %853 = llvm.add %852, %847  : i64
    %854 = llvm.getelementptr %arg4[%853] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %855 = llvm.load %854 : !llvm.ptr -> f32
    %856 = llvm.fptrunc %0 : f64 to f32
    %857 = llvm.fadd %70, %856  : f32
    %858 = llvm.fmul %857, %3  : f32
    %859 = llvm.bitcast %857 : f32 to i32
    %860 = llvm.lshr %859, %4  : i32
    %861 = llvm.sub %5, %860  : i32
    %862 = llvm.bitcast %861 : i32 to f32
    %863 = llvm.fmul %862, %862  : f32
    %864 = llvm.fmul %863, %858  : f32
    %865 = llvm.fsub %6, %864  : f32
    %866 = llvm.fmul %865, %863  : f32
    %867 = llvm.fsub %855, %69  : f32
    %868 = llvm.fmul %867, %866  : f32
    %869 = llvm.fmul %868, %67  : f32
    %870 = llvm.fadd %869, %68  : f32
    %871 = llvm.mul %64, %62  : i64
    %872 = llvm.mul %64, %61  : i64
    %873 = llvm.add %871, %872  : i64
    %874 = llvm.mul %65, %60  : i64
    %875 = llvm.add %873, %874  : i64
    %876 = llvm.add %875, %847  : i64
    %877 = llvm.getelementptr %arg5[%876] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %870, %877 : f32, !llvm.ptr
    %878 = llvm.add %71, %30  : i64
    %879 = llvm.mul %64, %62  : i64
    %880 = llvm.mul %64, %61  : i64
    %881 = llvm.add %879, %880  : i64
    %882 = llvm.mul %65, %60  : i64
    %883 = llvm.add %881, %882  : i64
    %884 = llvm.add %883, %878  : i64
    %885 = llvm.getelementptr %arg4[%884] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %886 = llvm.load %885 : !llvm.ptr -> f32
    %887 = llvm.fptrunc %0 : f64 to f32
    %888 = llvm.fadd %70, %887  : f32
    %889 = llvm.fmul %888, %3  : f32
    %890 = llvm.bitcast %888 : f32 to i32
    %891 = llvm.lshr %890, %4  : i32
    %892 = llvm.sub %5, %891  : i32
    %893 = llvm.bitcast %892 : i32 to f32
    %894 = llvm.fmul %893, %893  : f32
    %895 = llvm.fmul %894, %889  : f32
    %896 = llvm.fsub %6, %895  : f32
    %897 = llvm.fmul %896, %894  : f32
    %898 = llvm.fsub %886, %69  : f32
    %899 = llvm.fmul %898, %897  : f32
    %900 = llvm.fmul %899, %67  : f32
    %901 = llvm.fadd %900, %68  : f32
    %902 = llvm.mul %64, %62  : i64
    %903 = llvm.mul %64, %61  : i64
    %904 = llvm.add %902, %903  : i64
    %905 = llvm.mul %65, %60  : i64
    %906 = llvm.add %904, %905  : i64
    %907 = llvm.add %906, %878  : i64
    %908 = llvm.getelementptr %arg5[%907] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %901, %908 : f32, !llvm.ptr
    %909 = llvm.add %71, %31  : i64
    %910 = llvm.mul %64, %62  : i64
    %911 = llvm.mul %64, %61  : i64
    %912 = llvm.add %910, %911  : i64
    %913 = llvm.mul %65, %60  : i64
    %914 = llvm.add %912, %913  : i64
    %915 = llvm.add %914, %909  : i64
    %916 = llvm.getelementptr %arg4[%915] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %917 = llvm.load %916 : !llvm.ptr -> f32
    %918 = llvm.fptrunc %0 : f64 to f32
    %919 = llvm.fadd %70, %918  : f32
    %920 = llvm.fmul %919, %3  : f32
    %921 = llvm.bitcast %919 : f32 to i32
    %922 = llvm.lshr %921, %4  : i32
    %923 = llvm.sub %5, %922  : i32
    %924 = llvm.bitcast %923 : i32 to f32
    %925 = llvm.fmul %924, %924  : f32
    %926 = llvm.fmul %925, %920  : f32
    %927 = llvm.fsub %6, %926  : f32
    %928 = llvm.fmul %927, %925  : f32
    %929 = llvm.fsub %917, %69  : f32
    %930 = llvm.fmul %929, %928  : f32
    %931 = llvm.fmul %930, %67  : f32
    %932 = llvm.fadd %931, %68  : f32
    %933 = llvm.mul %64, %62  : i64
    %934 = llvm.mul %64, %61  : i64
    %935 = llvm.add %933, %934  : i64
    %936 = llvm.mul %65, %60  : i64
    %937 = llvm.add %935, %936  : i64
    %938 = llvm.add %937, %909  : i64
    %939 = llvm.getelementptr %arg5[%938] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %932, %939 : f32, !llvm.ptr
    %940 = llvm.add %71, %32  : i64
    %941 = llvm.mul %64, %62  : i64
    %942 = llvm.mul %64, %61  : i64
    %943 = llvm.add %941, %942  : i64
    %944 = llvm.mul %65, %60  : i64
    %945 = llvm.add %943, %944  : i64
    %946 = llvm.add %945, %940  : i64
    %947 = llvm.getelementptr %arg4[%946] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %948 = llvm.load %947 : !llvm.ptr -> f32
    %949 = llvm.fptrunc %0 : f64 to f32
    %950 = llvm.fadd %70, %949  : f32
    %951 = llvm.fmul %950, %3  : f32
    %952 = llvm.bitcast %950 : f32 to i32
    %953 = llvm.lshr %952, %4  : i32
    %954 = llvm.sub %5, %953  : i32
    %955 = llvm.bitcast %954 : i32 to f32
    %956 = llvm.fmul %955, %955  : f32
    %957 = llvm.fmul %956, %951  : f32
    %958 = llvm.fsub %6, %957  : f32
    %959 = llvm.fmul %958, %956  : f32
    %960 = llvm.fsub %948, %69  : f32
    %961 = llvm.fmul %960, %959  : f32
    %962 = llvm.fmul %961, %67  : f32
    %963 = llvm.fadd %962, %68  : f32
    %964 = llvm.mul %64, %62  : i64
    %965 = llvm.mul %64, %61  : i64
    %966 = llvm.add %964, %965  : i64
    %967 = llvm.mul %65, %60  : i64
    %968 = llvm.add %966, %967  : i64
    %969 = llvm.add %968, %940  : i64
    %970 = llvm.getelementptr %arg5[%969] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %963, %970 : f32, !llvm.ptr
    %971 = llvm.add %71, %33  : i64
    %972 = llvm.mul %64, %62  : i64
    %973 = llvm.mul %64, %61  : i64
    %974 = llvm.add %972, %973  : i64
    %975 = llvm.mul %65, %60  : i64
    %976 = llvm.add %974, %975  : i64
    %977 = llvm.add %976, %971  : i64
    %978 = llvm.getelementptr %arg4[%977] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %979 = llvm.load %978 : !llvm.ptr -> f32
    %980 = llvm.fptrunc %0 : f64 to f32
    %981 = llvm.fadd %70, %980  : f32
    %982 = llvm.fmul %981, %3  : f32
    %983 = llvm.bitcast %981 : f32 to i32
    %984 = llvm.lshr %983, %4  : i32
    %985 = llvm.sub %5, %984  : i32
    %986 = llvm.bitcast %985 : i32 to f32
    %987 = llvm.fmul %986, %986  : f32
    %988 = llvm.fmul %987, %982  : f32
    %989 = llvm.fsub %6, %988  : f32
    %990 = llvm.fmul %989, %987  : f32
    %991 = llvm.fsub %979, %69  : f32
    %992 = llvm.fmul %991, %990  : f32
    %993 = llvm.fmul %992, %67  : f32
    %994 = llvm.fadd %993, %68  : f32
    %995 = llvm.mul %64, %62  : i64
    %996 = llvm.mul %64, %61  : i64
    %997 = llvm.add %995, %996  : i64
    %998 = llvm.mul %65, %60  : i64
    %999 = llvm.add %997, %998  : i64
    %1000 = llvm.add %999, %971  : i64
    %1001 = llvm.getelementptr %arg5[%1000] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %994, %1001 : f32, !llvm.ptr
    %1002 = llvm.add %71, %34  : i64
    %1003 = llvm.mul %64, %62  : i64
    %1004 = llvm.mul %64, %61  : i64
    %1005 = llvm.add %1003, %1004  : i64
    %1006 = llvm.mul %65, %60  : i64
    %1007 = llvm.add %1005, %1006  : i64
    %1008 = llvm.add %1007, %1002  : i64
    %1009 = llvm.getelementptr %arg4[%1008] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1010 = llvm.load %1009 : !llvm.ptr -> f32
    %1011 = llvm.fptrunc %0 : f64 to f32
    %1012 = llvm.fadd %70, %1011  : f32
    %1013 = llvm.fmul %1012, %3  : f32
    %1014 = llvm.bitcast %1012 : f32 to i32
    %1015 = llvm.lshr %1014, %4  : i32
    %1016 = llvm.sub %5, %1015  : i32
    %1017 = llvm.bitcast %1016 : i32 to f32
    %1018 = llvm.fmul %1017, %1017  : f32
    %1019 = llvm.fmul %1018, %1013  : f32
    %1020 = llvm.fsub %6, %1019  : f32
    %1021 = llvm.fmul %1020, %1018  : f32
    %1022 = llvm.fsub %1010, %69  : f32
    %1023 = llvm.fmul %1022, %1021  : f32
    %1024 = llvm.fmul %1023, %67  : f32
    %1025 = llvm.fadd %1024, %68  : f32
    %1026 = llvm.mul %64, %62  : i64
    %1027 = llvm.mul %64, %61  : i64
    %1028 = llvm.add %1026, %1027  : i64
    %1029 = llvm.mul %65, %60  : i64
    %1030 = llvm.add %1028, %1029  : i64
    %1031 = llvm.add %1030, %1002  : i64
    %1032 = llvm.getelementptr %arg5[%1031] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1025, %1032 : f32, !llvm.ptr
    %1033 = llvm.add %71, %35  : i64
    %1034 = llvm.mul %64, %62  : i64
    %1035 = llvm.mul %64, %61  : i64
    %1036 = llvm.add %1034, %1035  : i64
    %1037 = llvm.mul %65, %60  : i64
    %1038 = llvm.add %1036, %1037  : i64
    %1039 = llvm.add %1038, %1033  : i64
    %1040 = llvm.getelementptr %arg4[%1039] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1041 = llvm.load %1040 : !llvm.ptr -> f32
    %1042 = llvm.fptrunc %0 : f64 to f32
    %1043 = llvm.fadd %70, %1042  : f32
    %1044 = llvm.fmul %1043, %3  : f32
    %1045 = llvm.bitcast %1043 : f32 to i32
    %1046 = llvm.lshr %1045, %4  : i32
    %1047 = llvm.sub %5, %1046  : i32
    %1048 = llvm.bitcast %1047 : i32 to f32
    %1049 = llvm.fmul %1048, %1048  : f32
    %1050 = llvm.fmul %1049, %1044  : f32
    %1051 = llvm.fsub %6, %1050  : f32
    %1052 = llvm.fmul %1051, %1049  : f32
    %1053 = llvm.fsub %1041, %69  : f32
    %1054 = llvm.fmul %1053, %1052  : f32
    %1055 = llvm.fmul %1054, %67  : f32
    %1056 = llvm.fadd %1055, %68  : f32
    %1057 = llvm.mul %64, %62  : i64
    %1058 = llvm.mul %64, %61  : i64
    %1059 = llvm.add %1057, %1058  : i64
    %1060 = llvm.mul %65, %60  : i64
    %1061 = llvm.add %1059, %1060  : i64
    %1062 = llvm.add %1061, %1033  : i64
    %1063 = llvm.getelementptr %arg5[%1062] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1056, %1063 : f32, !llvm.ptr
    %1064 = llvm.add %71, %36  : i64
    %1065 = llvm.mul %64, %62  : i64
    %1066 = llvm.mul %64, %61  : i64
    %1067 = llvm.add %1065, %1066  : i64
    %1068 = llvm.mul %65, %60  : i64
    %1069 = llvm.add %1067, %1068  : i64
    %1070 = llvm.add %1069, %1064  : i64
    %1071 = llvm.getelementptr %arg4[%1070] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1072 = llvm.load %1071 : !llvm.ptr -> f32
    %1073 = llvm.fptrunc %0 : f64 to f32
    %1074 = llvm.fadd %70, %1073  : f32
    %1075 = llvm.fmul %1074, %3  : f32
    %1076 = llvm.bitcast %1074 : f32 to i32
    %1077 = llvm.lshr %1076, %4  : i32
    %1078 = llvm.sub %5, %1077  : i32
    %1079 = llvm.bitcast %1078 : i32 to f32
    %1080 = llvm.fmul %1079, %1079  : f32
    %1081 = llvm.fmul %1080, %1075  : f32
    %1082 = llvm.fsub %6, %1081  : f32
    %1083 = llvm.fmul %1082, %1080  : f32
    %1084 = llvm.fsub %1072, %69  : f32
    %1085 = llvm.fmul %1084, %1083  : f32
    %1086 = llvm.fmul %1085, %67  : f32
    %1087 = llvm.fadd %1086, %68  : f32
    %1088 = llvm.mul %64, %62  : i64
    %1089 = llvm.mul %64, %61  : i64
    %1090 = llvm.add %1088, %1089  : i64
    %1091 = llvm.mul %65, %60  : i64
    %1092 = llvm.add %1090, %1091  : i64
    %1093 = llvm.add %1092, %1064  : i64
    %1094 = llvm.getelementptr %arg5[%1093] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1087, %1094 : f32, !llvm.ptr
    %1095 = llvm.add %71, %37  : i64
    %1096 = llvm.mul %64, %62  : i64
    %1097 = llvm.mul %64, %61  : i64
    %1098 = llvm.add %1096, %1097  : i64
    %1099 = llvm.mul %65, %60  : i64
    %1100 = llvm.add %1098, %1099  : i64
    %1101 = llvm.add %1100, %1095  : i64
    %1102 = llvm.getelementptr %arg4[%1101] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1103 = llvm.load %1102 : !llvm.ptr -> f32
    %1104 = llvm.fptrunc %0 : f64 to f32
    %1105 = llvm.fadd %70, %1104  : f32
    %1106 = llvm.fmul %1105, %3  : f32
    %1107 = llvm.bitcast %1105 : f32 to i32
    %1108 = llvm.lshr %1107, %4  : i32
    %1109 = llvm.sub %5, %1108  : i32
    %1110 = llvm.bitcast %1109 : i32 to f32
    %1111 = llvm.fmul %1110, %1110  : f32
    %1112 = llvm.fmul %1111, %1106  : f32
    %1113 = llvm.fsub %6, %1112  : f32
    %1114 = llvm.fmul %1113, %1111  : f32
    %1115 = llvm.fsub %1103, %69  : f32
    %1116 = llvm.fmul %1115, %1114  : f32
    %1117 = llvm.fmul %1116, %67  : f32
    %1118 = llvm.fadd %1117, %68  : f32
    %1119 = llvm.mul %64, %62  : i64
    %1120 = llvm.mul %64, %61  : i64
    %1121 = llvm.add %1119, %1120  : i64
    %1122 = llvm.mul %65, %60  : i64
    %1123 = llvm.add %1121, %1122  : i64
    %1124 = llvm.add %1123, %1095  : i64
    %1125 = llvm.getelementptr %arg5[%1124] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1118, %1125 : f32, !llvm.ptr
    %1126 = llvm.add %71, %38  : i64
    %1127 = llvm.mul %64, %62  : i64
    %1128 = llvm.mul %64, %61  : i64
    %1129 = llvm.add %1127, %1128  : i64
    %1130 = llvm.mul %65, %60  : i64
    %1131 = llvm.add %1129, %1130  : i64
    %1132 = llvm.add %1131, %1126  : i64
    %1133 = llvm.getelementptr %arg4[%1132] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1134 = llvm.load %1133 : !llvm.ptr -> f32
    %1135 = llvm.fptrunc %0 : f64 to f32
    %1136 = llvm.fadd %70, %1135  : f32
    %1137 = llvm.fmul %1136, %3  : f32
    %1138 = llvm.bitcast %1136 : f32 to i32
    %1139 = llvm.lshr %1138, %4  : i32
    %1140 = llvm.sub %5, %1139  : i32
    %1141 = llvm.bitcast %1140 : i32 to f32
    %1142 = llvm.fmul %1141, %1141  : f32
    %1143 = llvm.fmul %1142, %1137  : f32
    %1144 = llvm.fsub %6, %1143  : f32
    %1145 = llvm.fmul %1144, %1142  : f32
    %1146 = llvm.fsub %1134, %69  : f32
    %1147 = llvm.fmul %1146, %1145  : f32
    %1148 = llvm.fmul %1147, %67  : f32
    %1149 = llvm.fadd %1148, %68  : f32
    %1150 = llvm.mul %64, %62  : i64
    %1151 = llvm.mul %64, %61  : i64
    %1152 = llvm.add %1150, %1151  : i64
    %1153 = llvm.mul %65, %60  : i64
    %1154 = llvm.add %1152, %1153  : i64
    %1155 = llvm.add %1154, %1126  : i64
    %1156 = llvm.getelementptr %arg5[%1155] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1149, %1156 : f32, !llvm.ptr
    %1157 = llvm.add %71, %39  : i64
    %1158 = llvm.mul %64, %62  : i64
    %1159 = llvm.mul %64, %61  : i64
    %1160 = llvm.add %1158, %1159  : i64
    %1161 = llvm.mul %65, %60  : i64
    %1162 = llvm.add %1160, %1161  : i64
    %1163 = llvm.add %1162, %1157  : i64
    %1164 = llvm.getelementptr %arg4[%1163] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1165 = llvm.load %1164 : !llvm.ptr -> f32
    %1166 = llvm.fptrunc %0 : f64 to f32
    %1167 = llvm.fadd %70, %1166  : f32
    %1168 = llvm.fmul %1167, %3  : f32
    %1169 = llvm.bitcast %1167 : f32 to i32
    %1170 = llvm.lshr %1169, %4  : i32
    %1171 = llvm.sub %5, %1170  : i32
    %1172 = llvm.bitcast %1171 : i32 to f32
    %1173 = llvm.fmul %1172, %1172  : f32
    %1174 = llvm.fmul %1173, %1168  : f32
    %1175 = llvm.fsub %6, %1174  : f32
    %1176 = llvm.fmul %1175, %1173  : f32
    %1177 = llvm.fsub %1165, %69  : f32
    %1178 = llvm.fmul %1177, %1176  : f32
    %1179 = llvm.fmul %1178, %67  : f32
    %1180 = llvm.fadd %1179, %68  : f32
    %1181 = llvm.mul %64, %62  : i64
    %1182 = llvm.mul %64, %61  : i64
    %1183 = llvm.add %1181, %1182  : i64
    %1184 = llvm.mul %65, %60  : i64
    %1185 = llvm.add %1183, %1184  : i64
    %1186 = llvm.add %1185, %1157  : i64
    %1187 = llvm.getelementptr %arg5[%1186] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1180, %1187 : f32, !llvm.ptr
    %1188 = llvm.add %71, %40  : i64
    %1189 = llvm.mul %64, %62  : i64
    %1190 = llvm.mul %64, %61  : i64
    %1191 = llvm.add %1189, %1190  : i64
    %1192 = llvm.mul %65, %60  : i64
    %1193 = llvm.add %1191, %1192  : i64
    %1194 = llvm.add %1193, %1188  : i64
    %1195 = llvm.getelementptr %arg4[%1194] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1196 = llvm.load %1195 : !llvm.ptr -> f32
    %1197 = llvm.fptrunc %0 : f64 to f32
    %1198 = llvm.fadd %70, %1197  : f32
    %1199 = llvm.fmul %1198, %3  : f32
    %1200 = llvm.bitcast %1198 : f32 to i32
    %1201 = llvm.lshr %1200, %4  : i32
    %1202 = llvm.sub %5, %1201  : i32
    %1203 = llvm.bitcast %1202 : i32 to f32
    %1204 = llvm.fmul %1203, %1203  : f32
    %1205 = llvm.fmul %1204, %1199  : f32
    %1206 = llvm.fsub %6, %1205  : f32
    %1207 = llvm.fmul %1206, %1204  : f32
    %1208 = llvm.fsub %1196, %69  : f32
    %1209 = llvm.fmul %1208, %1207  : f32
    %1210 = llvm.fmul %1209, %67  : f32
    %1211 = llvm.fadd %1210, %68  : f32
    %1212 = llvm.mul %64, %62  : i64
    %1213 = llvm.mul %64, %61  : i64
    %1214 = llvm.add %1212, %1213  : i64
    %1215 = llvm.mul %65, %60  : i64
    %1216 = llvm.add %1214, %1215  : i64
    %1217 = llvm.add %1216, %1188  : i64
    %1218 = llvm.getelementptr %arg5[%1217] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1211, %1218 : f32, !llvm.ptr
    %1219 = llvm.add %71, %41  : i64
    %1220 = llvm.mul %64, %62  : i64
    %1221 = llvm.mul %64, %61  : i64
    %1222 = llvm.add %1220, %1221  : i64
    %1223 = llvm.mul %65, %60  : i64
    %1224 = llvm.add %1222, %1223  : i64
    %1225 = llvm.add %1224, %1219  : i64
    %1226 = llvm.getelementptr %arg4[%1225] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1227 = llvm.load %1226 : !llvm.ptr -> f32
    %1228 = llvm.fptrunc %0 : f64 to f32
    %1229 = llvm.fadd %70, %1228  : f32
    %1230 = llvm.fmul %1229, %3  : f32
    %1231 = llvm.bitcast %1229 : f32 to i32
    %1232 = llvm.lshr %1231, %4  : i32
    %1233 = llvm.sub %5, %1232  : i32
    %1234 = llvm.bitcast %1233 : i32 to f32
    %1235 = llvm.fmul %1234, %1234  : f32
    %1236 = llvm.fmul %1235, %1230  : f32
    %1237 = llvm.fsub %6, %1236  : f32
    %1238 = llvm.fmul %1237, %1235  : f32
    %1239 = llvm.fsub %1227, %69  : f32
    %1240 = llvm.fmul %1239, %1238  : f32
    %1241 = llvm.fmul %1240, %67  : f32
    %1242 = llvm.fadd %1241, %68  : f32
    %1243 = llvm.mul %64, %62  : i64
    %1244 = llvm.mul %64, %61  : i64
    %1245 = llvm.add %1243, %1244  : i64
    %1246 = llvm.mul %65, %60  : i64
    %1247 = llvm.add %1245, %1246  : i64
    %1248 = llvm.add %1247, %1219  : i64
    %1249 = llvm.getelementptr %arg5[%1248] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1242, %1249 : f32, !llvm.ptr
    %1250 = llvm.add %71, %42  : i64
    %1251 = llvm.mul %64, %62  : i64
    %1252 = llvm.mul %64, %61  : i64
    %1253 = llvm.add %1251, %1252  : i64
    %1254 = llvm.mul %65, %60  : i64
    %1255 = llvm.add %1253, %1254  : i64
    %1256 = llvm.add %1255, %1250  : i64
    %1257 = llvm.getelementptr %arg4[%1256] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1258 = llvm.load %1257 : !llvm.ptr -> f32
    %1259 = llvm.fptrunc %0 : f64 to f32
    %1260 = llvm.fadd %70, %1259  : f32
    %1261 = llvm.fmul %1260, %3  : f32
    %1262 = llvm.bitcast %1260 : f32 to i32
    %1263 = llvm.lshr %1262, %4  : i32
    %1264 = llvm.sub %5, %1263  : i32
    %1265 = llvm.bitcast %1264 : i32 to f32
    %1266 = llvm.fmul %1265, %1265  : f32
    %1267 = llvm.fmul %1266, %1261  : f32
    %1268 = llvm.fsub %6, %1267  : f32
    %1269 = llvm.fmul %1268, %1266  : f32
    %1270 = llvm.fsub %1258, %69  : f32
    %1271 = llvm.fmul %1270, %1269  : f32
    %1272 = llvm.fmul %1271, %67  : f32
    %1273 = llvm.fadd %1272, %68  : f32
    %1274 = llvm.mul %64, %62  : i64
    %1275 = llvm.mul %64, %61  : i64
    %1276 = llvm.add %1274, %1275  : i64
    %1277 = llvm.mul %65, %60  : i64
    %1278 = llvm.add %1276, %1277  : i64
    %1279 = llvm.add %1278, %1250  : i64
    %1280 = llvm.getelementptr %arg5[%1279] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1273, %1280 : f32, !llvm.ptr
    %1281 = llvm.add %71, %43  : i64
    %1282 = llvm.mul %64, %62  : i64
    %1283 = llvm.mul %64, %61  : i64
    %1284 = llvm.add %1282, %1283  : i64
    %1285 = llvm.mul %65, %60  : i64
    %1286 = llvm.add %1284, %1285  : i64
    %1287 = llvm.add %1286, %1281  : i64
    %1288 = llvm.getelementptr %arg4[%1287] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1289 = llvm.load %1288 : !llvm.ptr -> f32
    %1290 = llvm.fptrunc %0 : f64 to f32
    %1291 = llvm.fadd %70, %1290  : f32
    %1292 = llvm.fmul %1291, %3  : f32
    %1293 = llvm.bitcast %1291 : f32 to i32
    %1294 = llvm.lshr %1293, %4  : i32
    %1295 = llvm.sub %5, %1294  : i32
    %1296 = llvm.bitcast %1295 : i32 to f32
    %1297 = llvm.fmul %1296, %1296  : f32
    %1298 = llvm.fmul %1297, %1292  : f32
    %1299 = llvm.fsub %6, %1298  : f32
    %1300 = llvm.fmul %1299, %1297  : f32
    %1301 = llvm.fsub %1289, %69  : f32
    %1302 = llvm.fmul %1301, %1300  : f32
    %1303 = llvm.fmul %1302, %67  : f32
    %1304 = llvm.fadd %1303, %68  : f32
    %1305 = llvm.mul %64, %62  : i64
    %1306 = llvm.mul %64, %61  : i64
    %1307 = llvm.add %1305, %1306  : i64
    %1308 = llvm.mul %65, %60  : i64
    %1309 = llvm.add %1307, %1308  : i64
    %1310 = llvm.add %1309, %1281  : i64
    %1311 = llvm.getelementptr %arg5[%1310] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1304, %1311 : f32, !llvm.ptr
    %1312 = llvm.add %71, %44  : i64
    %1313 = llvm.mul %64, %62  : i64
    %1314 = llvm.mul %64, %61  : i64
    %1315 = llvm.add %1313, %1314  : i64
    %1316 = llvm.mul %65, %60  : i64
    %1317 = llvm.add %1315, %1316  : i64
    %1318 = llvm.add %1317, %1312  : i64
    %1319 = llvm.getelementptr %arg4[%1318] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1320 = llvm.load %1319 : !llvm.ptr -> f32
    %1321 = llvm.fptrunc %0 : f64 to f32
    %1322 = llvm.fadd %70, %1321  : f32
    %1323 = llvm.fmul %1322, %3  : f32
    %1324 = llvm.bitcast %1322 : f32 to i32
    %1325 = llvm.lshr %1324, %4  : i32
    %1326 = llvm.sub %5, %1325  : i32
    %1327 = llvm.bitcast %1326 : i32 to f32
    %1328 = llvm.fmul %1327, %1327  : f32
    %1329 = llvm.fmul %1328, %1323  : f32
    %1330 = llvm.fsub %6, %1329  : f32
    %1331 = llvm.fmul %1330, %1328  : f32
    %1332 = llvm.fsub %1320, %69  : f32
    %1333 = llvm.fmul %1332, %1331  : f32
    %1334 = llvm.fmul %1333, %67  : f32
    %1335 = llvm.fadd %1334, %68  : f32
    %1336 = llvm.mul %64, %62  : i64
    %1337 = llvm.mul %64, %61  : i64
    %1338 = llvm.add %1336, %1337  : i64
    %1339 = llvm.mul %65, %60  : i64
    %1340 = llvm.add %1338, %1339  : i64
    %1341 = llvm.add %1340, %1312  : i64
    %1342 = llvm.getelementptr %arg5[%1341] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1335, %1342 : f32, !llvm.ptr
    %1343 = llvm.add %71, %45  : i64
    %1344 = llvm.mul %64, %62  : i64
    %1345 = llvm.mul %64, %61  : i64
    %1346 = llvm.add %1344, %1345  : i64
    %1347 = llvm.mul %65, %60  : i64
    %1348 = llvm.add %1346, %1347  : i64
    %1349 = llvm.add %1348, %1343  : i64
    %1350 = llvm.getelementptr %arg4[%1349] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1351 = llvm.load %1350 : !llvm.ptr -> f32
    %1352 = llvm.fptrunc %0 : f64 to f32
    %1353 = llvm.fadd %70, %1352  : f32
    %1354 = llvm.fmul %1353, %3  : f32
    %1355 = llvm.bitcast %1353 : f32 to i32
    %1356 = llvm.lshr %1355, %4  : i32
    %1357 = llvm.sub %5, %1356  : i32
    %1358 = llvm.bitcast %1357 : i32 to f32
    %1359 = llvm.fmul %1358, %1358  : f32
    %1360 = llvm.fmul %1359, %1354  : f32
    %1361 = llvm.fsub %6, %1360  : f32
    %1362 = llvm.fmul %1361, %1359  : f32
    %1363 = llvm.fsub %1351, %69  : f32
    %1364 = llvm.fmul %1363, %1362  : f32
    %1365 = llvm.fmul %1364, %67  : f32
    %1366 = llvm.fadd %1365, %68  : f32
    %1367 = llvm.mul %64, %62  : i64
    %1368 = llvm.mul %64, %61  : i64
    %1369 = llvm.add %1367, %1368  : i64
    %1370 = llvm.mul %65, %60  : i64
    %1371 = llvm.add %1369, %1370  : i64
    %1372 = llvm.add %1371, %1343  : i64
    %1373 = llvm.getelementptr %arg5[%1372] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1366, %1373 : f32, !llvm.ptr
    %1374 = llvm.add %71, %46  : i64
    %1375 = llvm.mul %64, %62  : i64
    %1376 = llvm.mul %64, %61  : i64
    %1377 = llvm.add %1375, %1376  : i64
    %1378 = llvm.mul %65, %60  : i64
    %1379 = llvm.add %1377, %1378  : i64
    %1380 = llvm.add %1379, %1374  : i64
    %1381 = llvm.getelementptr %arg4[%1380] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1382 = llvm.load %1381 : !llvm.ptr -> f32
    %1383 = llvm.fptrunc %0 : f64 to f32
    %1384 = llvm.fadd %70, %1383  : f32
    %1385 = llvm.fmul %1384, %3  : f32
    %1386 = llvm.bitcast %1384 : f32 to i32
    %1387 = llvm.lshr %1386, %4  : i32
    %1388 = llvm.sub %5, %1387  : i32
    %1389 = llvm.bitcast %1388 : i32 to f32
    %1390 = llvm.fmul %1389, %1389  : f32
    %1391 = llvm.fmul %1390, %1385  : f32
    %1392 = llvm.fsub %6, %1391  : f32
    %1393 = llvm.fmul %1392, %1390  : f32
    %1394 = llvm.fsub %1382, %69  : f32
    %1395 = llvm.fmul %1394, %1393  : f32
    %1396 = llvm.fmul %1395, %67  : f32
    %1397 = llvm.fadd %1396, %68  : f32
    %1398 = llvm.mul %64, %62  : i64
    %1399 = llvm.mul %64, %61  : i64
    %1400 = llvm.add %1398, %1399  : i64
    %1401 = llvm.mul %65, %60  : i64
    %1402 = llvm.add %1400, %1401  : i64
    %1403 = llvm.add %1402, %1374  : i64
    %1404 = llvm.getelementptr %arg5[%1403] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1397, %1404 : f32, !llvm.ptr
    %1405 = llvm.add %71, %47  : i64
    %1406 = llvm.mul %64, %62  : i64
    %1407 = llvm.mul %64, %61  : i64
    %1408 = llvm.add %1406, %1407  : i64
    %1409 = llvm.mul %65, %60  : i64
    %1410 = llvm.add %1408, %1409  : i64
    %1411 = llvm.add %1410, %1405  : i64
    %1412 = llvm.getelementptr %arg4[%1411] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1413 = llvm.load %1412 : !llvm.ptr -> f32
    %1414 = llvm.fptrunc %0 : f64 to f32
    %1415 = llvm.fadd %70, %1414  : f32
    %1416 = llvm.fmul %1415, %3  : f32
    %1417 = llvm.bitcast %1415 : f32 to i32
    %1418 = llvm.lshr %1417, %4  : i32
    %1419 = llvm.sub %5, %1418  : i32
    %1420 = llvm.bitcast %1419 : i32 to f32
    %1421 = llvm.fmul %1420, %1420  : f32
    %1422 = llvm.fmul %1421, %1416  : f32
    %1423 = llvm.fsub %6, %1422  : f32
    %1424 = llvm.fmul %1423, %1421  : f32
    %1425 = llvm.fsub %1413, %69  : f32
    %1426 = llvm.fmul %1425, %1424  : f32
    %1427 = llvm.fmul %1426, %67  : f32
    %1428 = llvm.fadd %1427, %68  : f32
    %1429 = llvm.mul %64, %62  : i64
    %1430 = llvm.mul %64, %61  : i64
    %1431 = llvm.add %1429, %1430  : i64
    %1432 = llvm.mul %65, %60  : i64
    %1433 = llvm.add %1431, %1432  : i64
    %1434 = llvm.add %1433, %1405  : i64
    %1435 = llvm.getelementptr %arg5[%1434] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1428, %1435 : f32, !llvm.ptr
    %1436 = llvm.add %71, %48  : i64
    %1437 = llvm.mul %64, %62  : i64
    %1438 = llvm.mul %64, %61  : i64
    %1439 = llvm.add %1437, %1438  : i64
    %1440 = llvm.mul %65, %60  : i64
    %1441 = llvm.add %1439, %1440  : i64
    %1442 = llvm.add %1441, %1436  : i64
    %1443 = llvm.getelementptr %arg4[%1442] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1444 = llvm.load %1443 : !llvm.ptr -> f32
    %1445 = llvm.fptrunc %0 : f64 to f32
    %1446 = llvm.fadd %70, %1445  : f32
    %1447 = llvm.fmul %1446, %3  : f32
    %1448 = llvm.bitcast %1446 : f32 to i32
    %1449 = llvm.lshr %1448, %4  : i32
    %1450 = llvm.sub %5, %1449  : i32
    %1451 = llvm.bitcast %1450 : i32 to f32
    %1452 = llvm.fmul %1451, %1451  : f32
    %1453 = llvm.fmul %1452, %1447  : f32
    %1454 = llvm.fsub %6, %1453  : f32
    %1455 = llvm.fmul %1454, %1452  : f32
    %1456 = llvm.fsub %1444, %69  : f32
    %1457 = llvm.fmul %1456, %1455  : f32
    %1458 = llvm.fmul %1457, %67  : f32
    %1459 = llvm.fadd %1458, %68  : f32
    %1460 = llvm.mul %64, %62  : i64
    %1461 = llvm.mul %64, %61  : i64
    %1462 = llvm.add %1460, %1461  : i64
    %1463 = llvm.mul %65, %60  : i64
    %1464 = llvm.add %1462, %1463  : i64
    %1465 = llvm.add %1464, %1436  : i64
    %1466 = llvm.getelementptr %arg5[%1465] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1459, %1466 : f32, !llvm.ptr
    %1467 = llvm.add %71, %49  : i64
    %1468 = llvm.mul %64, %62  : i64
    %1469 = llvm.mul %64, %61  : i64
    %1470 = llvm.add %1468, %1469  : i64
    %1471 = llvm.mul %65, %60  : i64
    %1472 = llvm.add %1470, %1471  : i64
    %1473 = llvm.add %1472, %1467  : i64
    %1474 = llvm.getelementptr %arg4[%1473] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1475 = llvm.load %1474 : !llvm.ptr -> f32
    %1476 = llvm.fptrunc %0 : f64 to f32
    %1477 = llvm.fadd %70, %1476  : f32
    %1478 = llvm.fmul %1477, %3  : f32
    %1479 = llvm.bitcast %1477 : f32 to i32
    %1480 = llvm.lshr %1479, %4  : i32
    %1481 = llvm.sub %5, %1480  : i32
    %1482 = llvm.bitcast %1481 : i32 to f32
    %1483 = llvm.fmul %1482, %1482  : f32
    %1484 = llvm.fmul %1483, %1478  : f32
    %1485 = llvm.fsub %6, %1484  : f32
    %1486 = llvm.fmul %1485, %1483  : f32
    %1487 = llvm.fsub %1475, %69  : f32
    %1488 = llvm.fmul %1487, %1486  : f32
    %1489 = llvm.fmul %1488, %67  : f32
    %1490 = llvm.fadd %1489, %68  : f32
    %1491 = llvm.mul %64, %62  : i64
    %1492 = llvm.mul %64, %61  : i64
    %1493 = llvm.add %1491, %1492  : i64
    %1494 = llvm.mul %65, %60  : i64
    %1495 = llvm.add %1493, %1494  : i64
    %1496 = llvm.add %1495, %1467  : i64
    %1497 = llvm.getelementptr %arg5[%1496] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1490, %1497 : f32, !llvm.ptr
    %1498 = llvm.add %71, %50  : i64
    %1499 = llvm.mul %64, %62  : i64
    %1500 = llvm.mul %64, %61  : i64
    %1501 = llvm.add %1499, %1500  : i64
    %1502 = llvm.mul %65, %60  : i64
    %1503 = llvm.add %1501, %1502  : i64
    %1504 = llvm.add %1503, %1498  : i64
    %1505 = llvm.getelementptr %arg4[%1504] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1506 = llvm.load %1505 : !llvm.ptr -> f32
    %1507 = llvm.fptrunc %0 : f64 to f32
    %1508 = llvm.fadd %70, %1507  : f32
    %1509 = llvm.fmul %1508, %3  : f32
    %1510 = llvm.bitcast %1508 : f32 to i32
    %1511 = llvm.lshr %1510, %4  : i32
    %1512 = llvm.sub %5, %1511  : i32
    %1513 = llvm.bitcast %1512 : i32 to f32
    %1514 = llvm.fmul %1513, %1513  : f32
    %1515 = llvm.fmul %1514, %1509  : f32
    %1516 = llvm.fsub %6, %1515  : f32
    %1517 = llvm.fmul %1516, %1514  : f32
    %1518 = llvm.fsub %1506, %69  : f32
    %1519 = llvm.fmul %1518, %1517  : f32
    %1520 = llvm.fmul %1519, %67  : f32
    %1521 = llvm.fadd %1520, %68  : f32
    %1522 = llvm.mul %64, %62  : i64
    %1523 = llvm.mul %64, %61  : i64
    %1524 = llvm.add %1522, %1523  : i64
    %1525 = llvm.mul %65, %60  : i64
    %1526 = llvm.add %1524, %1525  : i64
    %1527 = llvm.add %1526, %1498  : i64
    %1528 = llvm.getelementptr %arg5[%1527] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1521, %1528 : f32, !llvm.ptr
    %1529 = llvm.add %71, %51  : i64
    %1530 = llvm.mul %64, %62  : i64
    %1531 = llvm.mul %64, %61  : i64
    %1532 = llvm.add %1530, %1531  : i64
    %1533 = llvm.mul %65, %60  : i64
    %1534 = llvm.add %1532, %1533  : i64
    %1535 = llvm.add %1534, %1529  : i64
    %1536 = llvm.getelementptr %arg4[%1535] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1537 = llvm.load %1536 : !llvm.ptr -> f32
    %1538 = llvm.fptrunc %0 : f64 to f32
    %1539 = llvm.fadd %70, %1538  : f32
    %1540 = llvm.fmul %1539, %3  : f32
    %1541 = llvm.bitcast %1539 : f32 to i32
    %1542 = llvm.lshr %1541, %4  : i32
    %1543 = llvm.sub %5, %1542  : i32
    %1544 = llvm.bitcast %1543 : i32 to f32
    %1545 = llvm.fmul %1544, %1544  : f32
    %1546 = llvm.fmul %1545, %1540  : f32
    %1547 = llvm.fsub %6, %1546  : f32
    %1548 = llvm.fmul %1547, %1545  : f32
    %1549 = llvm.fsub %1537, %69  : f32
    %1550 = llvm.fmul %1549, %1548  : f32
    %1551 = llvm.fmul %1550, %67  : f32
    %1552 = llvm.fadd %1551, %68  : f32
    %1553 = llvm.mul %64, %62  : i64
    %1554 = llvm.mul %64, %61  : i64
    %1555 = llvm.add %1553, %1554  : i64
    %1556 = llvm.mul %65, %60  : i64
    %1557 = llvm.add %1555, %1556  : i64
    %1558 = llvm.add %1557, %1529  : i64
    %1559 = llvm.getelementptr %arg5[%1558] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1552, %1559 : f32, !llvm.ptr
    %1560 = llvm.add %71, %52  : i64
    %1561 = llvm.mul %64, %62  : i64
    %1562 = llvm.mul %64, %61  : i64
    %1563 = llvm.add %1561, %1562  : i64
    %1564 = llvm.mul %65, %60  : i64
    %1565 = llvm.add %1563, %1564  : i64
    %1566 = llvm.add %1565, %1560  : i64
    %1567 = llvm.getelementptr %arg4[%1566] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1568 = llvm.load %1567 : !llvm.ptr -> f32
    %1569 = llvm.fptrunc %0 : f64 to f32
    %1570 = llvm.fadd %70, %1569  : f32
    %1571 = llvm.fmul %1570, %3  : f32
    %1572 = llvm.bitcast %1570 : f32 to i32
    %1573 = llvm.lshr %1572, %4  : i32
    %1574 = llvm.sub %5, %1573  : i32
    %1575 = llvm.bitcast %1574 : i32 to f32
    %1576 = llvm.fmul %1575, %1575  : f32
    %1577 = llvm.fmul %1576, %1571  : f32
    %1578 = llvm.fsub %6, %1577  : f32
    %1579 = llvm.fmul %1578, %1576  : f32
    %1580 = llvm.fsub %1568, %69  : f32
    %1581 = llvm.fmul %1580, %1579  : f32
    %1582 = llvm.fmul %1581, %67  : f32
    %1583 = llvm.fadd %1582, %68  : f32
    %1584 = llvm.mul %64, %62  : i64
    %1585 = llvm.mul %64, %61  : i64
    %1586 = llvm.add %1584, %1585  : i64
    %1587 = llvm.mul %65, %60  : i64
    %1588 = llvm.add %1586, %1587  : i64
    %1589 = llvm.add %1588, %1560  : i64
    %1590 = llvm.getelementptr %arg5[%1589] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1583, %1590 : f32, !llvm.ptr
    %1591 = llvm.add %71, %53  : i64
    %1592 = llvm.mul %64, %62  : i64
    %1593 = llvm.mul %64, %61  : i64
    %1594 = llvm.add %1592, %1593  : i64
    %1595 = llvm.mul %65, %60  : i64
    %1596 = llvm.add %1594, %1595  : i64
    %1597 = llvm.add %1596, %1591  : i64
    %1598 = llvm.getelementptr %arg4[%1597] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1599 = llvm.load %1598 : !llvm.ptr -> f32
    %1600 = llvm.fptrunc %0 : f64 to f32
    %1601 = llvm.fadd %70, %1600  : f32
    %1602 = llvm.fmul %1601, %3  : f32
    %1603 = llvm.bitcast %1601 : f32 to i32
    %1604 = llvm.lshr %1603, %4  : i32
    %1605 = llvm.sub %5, %1604  : i32
    %1606 = llvm.bitcast %1605 : i32 to f32
    %1607 = llvm.fmul %1606, %1606  : f32
    %1608 = llvm.fmul %1607, %1602  : f32
    %1609 = llvm.fsub %6, %1608  : f32
    %1610 = llvm.fmul %1609, %1607  : f32
    %1611 = llvm.fsub %1599, %69  : f32
    %1612 = llvm.fmul %1611, %1610  : f32
    %1613 = llvm.fmul %1612, %67  : f32
    %1614 = llvm.fadd %1613, %68  : f32
    %1615 = llvm.mul %64, %62  : i64
    %1616 = llvm.mul %64, %61  : i64
    %1617 = llvm.add %1615, %1616  : i64
    %1618 = llvm.mul %65, %60  : i64
    %1619 = llvm.add %1617, %1618  : i64
    %1620 = llvm.add %1619, %1591  : i64
    %1621 = llvm.getelementptr %arg5[%1620] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1614, %1621 : f32, !llvm.ptr
    %1622 = llvm.add %71, %54  : i64
    %1623 = llvm.mul %64, %62  : i64
    %1624 = llvm.mul %64, %61  : i64
    %1625 = llvm.add %1623, %1624  : i64
    %1626 = llvm.mul %65, %60  : i64
    %1627 = llvm.add %1625, %1626  : i64
    %1628 = llvm.add %1627, %1622  : i64
    %1629 = llvm.getelementptr %arg4[%1628] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1630 = llvm.load %1629 : !llvm.ptr -> f32
    %1631 = llvm.fptrunc %0 : f64 to f32
    %1632 = llvm.fadd %70, %1631  : f32
    %1633 = llvm.fmul %1632, %3  : f32
    %1634 = llvm.bitcast %1632 : f32 to i32
    %1635 = llvm.lshr %1634, %4  : i32
    %1636 = llvm.sub %5, %1635  : i32
    %1637 = llvm.bitcast %1636 : i32 to f32
    %1638 = llvm.fmul %1637, %1637  : f32
    %1639 = llvm.fmul %1638, %1633  : f32
    %1640 = llvm.fsub %6, %1639  : f32
    %1641 = llvm.fmul %1640, %1638  : f32
    %1642 = llvm.fsub %1630, %69  : f32
    %1643 = llvm.fmul %1642, %1641  : f32
    %1644 = llvm.fmul %1643, %67  : f32
    %1645 = llvm.fadd %1644, %68  : f32
    %1646 = llvm.mul %64, %62  : i64
    %1647 = llvm.mul %64, %61  : i64
    %1648 = llvm.add %1646, %1647  : i64
    %1649 = llvm.mul %65, %60  : i64
    %1650 = llvm.add %1648, %1649  : i64
    %1651 = llvm.add %1650, %1622  : i64
    %1652 = llvm.getelementptr %arg5[%1651] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1645, %1652 : f32, !llvm.ptr
    %1653 = llvm.add %71, %55  : i64
    %1654 = llvm.mul %64, %62  : i64
    %1655 = llvm.mul %64, %61  : i64
    %1656 = llvm.add %1654, %1655  : i64
    %1657 = llvm.mul %65, %60  : i64
    %1658 = llvm.add %1656, %1657  : i64
    %1659 = llvm.add %1658, %1653  : i64
    %1660 = llvm.getelementptr %arg4[%1659] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1661 = llvm.load %1660 : !llvm.ptr -> f32
    %1662 = llvm.fptrunc %0 : f64 to f32
    %1663 = llvm.fadd %70, %1662  : f32
    %1664 = llvm.fmul %1663, %3  : f32
    %1665 = llvm.bitcast %1663 : f32 to i32
    %1666 = llvm.lshr %1665, %4  : i32
    %1667 = llvm.sub %5, %1666  : i32
    %1668 = llvm.bitcast %1667 : i32 to f32
    %1669 = llvm.fmul %1668, %1668  : f32
    %1670 = llvm.fmul %1669, %1664  : f32
    %1671 = llvm.fsub %6, %1670  : f32
    %1672 = llvm.fmul %1671, %1669  : f32
    %1673 = llvm.fsub %1661, %69  : f32
    %1674 = llvm.fmul %1673, %1672  : f32
    %1675 = llvm.fmul %1674, %67  : f32
    %1676 = llvm.fadd %1675, %68  : f32
    %1677 = llvm.mul %64, %62  : i64
    %1678 = llvm.mul %64, %61  : i64
    %1679 = llvm.add %1677, %1678  : i64
    %1680 = llvm.mul %65, %60  : i64
    %1681 = llvm.add %1679, %1680  : i64
    %1682 = llvm.add %1681, %1653  : i64
    %1683 = llvm.getelementptr %arg5[%1682] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1676, %1683 : f32, !llvm.ptr
    %1684 = llvm.add %71, %56  : i64
    %1685 = llvm.mul %64, %62  : i64
    %1686 = llvm.mul %64, %61  : i64
    %1687 = llvm.add %1685, %1686  : i64
    %1688 = llvm.mul %65, %60  : i64
    %1689 = llvm.add %1687, %1688  : i64
    %1690 = llvm.add %1689, %1684  : i64
    %1691 = llvm.getelementptr %arg4[%1690] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1692 = llvm.load %1691 : !llvm.ptr -> f32
    %1693 = llvm.fptrunc %0 : f64 to f32
    %1694 = llvm.fadd %70, %1693  : f32
    %1695 = llvm.fmul %1694, %3  : f32
    %1696 = llvm.bitcast %1694 : f32 to i32
    %1697 = llvm.lshr %1696, %4  : i32
    %1698 = llvm.sub %5, %1697  : i32
    %1699 = llvm.bitcast %1698 : i32 to f32
    %1700 = llvm.fmul %1699, %1699  : f32
    %1701 = llvm.fmul %1700, %1695  : f32
    %1702 = llvm.fsub %6, %1701  : f32
    %1703 = llvm.fmul %1702, %1700  : f32
    %1704 = llvm.fsub %1692, %69  : f32
    %1705 = llvm.fmul %1704, %1703  : f32
    %1706 = llvm.fmul %1705, %67  : f32
    %1707 = llvm.fadd %1706, %68  : f32
    %1708 = llvm.mul %64, %62  : i64
    %1709 = llvm.mul %64, %61  : i64
    %1710 = llvm.add %1708, %1709  : i64
    %1711 = llvm.mul %65, %60  : i64
    %1712 = llvm.add %1710, %1711  : i64
    %1713 = llvm.add %1712, %1684  : i64
    %1714 = llvm.getelementptr %arg5[%1713] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1707, %1714 : f32, !llvm.ptr
    %1715 = llvm.add %71, %57  : i64
    %1716 = llvm.mul %64, %62  : i64
    %1717 = llvm.mul %64, %61  : i64
    %1718 = llvm.add %1716, %1717  : i64
    %1719 = llvm.mul %65, %60  : i64
    %1720 = llvm.add %1718, %1719  : i64
    %1721 = llvm.add %1720, %1715  : i64
    %1722 = llvm.getelementptr %arg4[%1721] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1723 = llvm.load %1722 : !llvm.ptr -> f32
    %1724 = llvm.fptrunc %0 : f64 to f32
    %1725 = llvm.fadd %70, %1724  : f32
    %1726 = llvm.fmul %1725, %3  : f32
    %1727 = llvm.bitcast %1725 : f32 to i32
    %1728 = llvm.lshr %1727, %4  : i32
    %1729 = llvm.sub %5, %1728  : i32
    %1730 = llvm.bitcast %1729 : i32 to f32
    %1731 = llvm.fmul %1730, %1730  : f32
    %1732 = llvm.fmul %1731, %1726  : f32
    %1733 = llvm.fsub %6, %1732  : f32
    %1734 = llvm.fmul %1733, %1731  : f32
    %1735 = llvm.fsub %1723, %69  : f32
    %1736 = llvm.fmul %1735, %1734  : f32
    %1737 = llvm.fmul %1736, %67  : f32
    %1738 = llvm.fadd %1737, %68  : f32
    %1739 = llvm.mul %64, %62  : i64
    %1740 = llvm.mul %64, %61  : i64
    %1741 = llvm.add %1739, %1740  : i64
    %1742 = llvm.mul %65, %60  : i64
    %1743 = llvm.add %1741, %1742  : i64
    %1744 = llvm.add %1743, %1715  : i64
    %1745 = llvm.getelementptr %arg5[%1744] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1738, %1745 : f32, !llvm.ptr
    %1746 = llvm.add %71, %58  : i64
    %1747 = llvm.mul %64, %62  : i64
    %1748 = llvm.mul %64, %61  : i64
    %1749 = llvm.add %1747, %1748  : i64
    %1750 = llvm.mul %65, %60  : i64
    %1751 = llvm.add %1749, %1750  : i64
    %1752 = llvm.add %1751, %1746  : i64
    %1753 = llvm.getelementptr %arg4[%1752] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1754 = llvm.load %1753 : !llvm.ptr -> f32
    %1755 = llvm.fptrunc %0 : f64 to f32
    %1756 = llvm.fadd %70, %1755  : f32
    %1757 = llvm.fmul %1756, %3  : f32
    %1758 = llvm.bitcast %1756 : f32 to i32
    %1759 = llvm.lshr %1758, %4  : i32
    %1760 = llvm.sub %5, %1759  : i32
    %1761 = llvm.bitcast %1760 : i32 to f32
    %1762 = llvm.fmul %1761, %1761  : f32
    %1763 = llvm.fmul %1762, %1757  : f32
    %1764 = llvm.fsub %6, %1763  : f32
    %1765 = llvm.fmul %1764, %1762  : f32
    %1766 = llvm.fsub %1754, %69  : f32
    %1767 = llvm.fmul %1766, %1765  : f32
    %1768 = llvm.fmul %1767, %67  : f32
    %1769 = llvm.fadd %1768, %68  : f32
    %1770 = llvm.mul %64, %62  : i64
    %1771 = llvm.mul %64, %61  : i64
    %1772 = llvm.add %1770, %1771  : i64
    %1773 = llvm.mul %65, %60  : i64
    %1774 = llvm.add %1772, %1773  : i64
    %1775 = llvm.add %1774, %1746  : i64
    %1776 = llvm.getelementptr %arg5[%1775] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1769, %1776 : f32, !llvm.ptr
    %1777 = llvm.add %71, %59  : i64
    %1778 = llvm.mul %64, %62  : i64
    %1779 = llvm.mul %64, %61  : i64
    %1780 = llvm.add %1778, %1779  : i64
    %1781 = llvm.mul %65, %60  : i64
    %1782 = llvm.add %1780, %1781  : i64
    %1783 = llvm.add %1782, %1777  : i64
    %1784 = llvm.getelementptr %arg4[%1783] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    %1785 = llvm.load %1784 : !llvm.ptr -> f32
    %1786 = llvm.fptrunc %0 : f64 to f32
    %1787 = llvm.fadd %70, %1786  : f32
    %1788 = llvm.fmul %1787, %3  : f32
    %1789 = llvm.bitcast %1787 : f32 to i32
    %1790 = llvm.lshr %1789, %4  : i32
    %1791 = llvm.sub %5, %1790  : i32
    %1792 = llvm.bitcast %1791 : i32 to f32
    %1793 = llvm.fmul %1792, %1792  : f32
    %1794 = llvm.fmul %1793, %1788  : f32
    %1795 = llvm.fsub %6, %1794  : f32
    %1796 = llvm.fmul %1795, %1793  : f32
    %1797 = llvm.fsub %1785, %69  : f32
    %1798 = llvm.fmul %1797, %1796  : f32
    %1799 = llvm.fmul %1798, %67  : f32
    %1800 = llvm.fadd %1799, %68  : f32
    %1801 = llvm.mul %64, %62  : i64
    %1802 = llvm.mul %64, %61  : i64
    %1803 = llvm.add %1801, %1802  : i64
    %1804 = llvm.mul %65, %60  : i64
    %1805 = llvm.add %1803, %1804  : i64
    %1806 = llvm.add %1805, %1777  : i64
    %1807 = llvm.getelementptr %arg5[%1806] : (!llvm.ptr, i64) -> !llvm.ptr, f32
    llvm.store %1800, %1807 : f32, !llvm.ptr
    %1808 = llvm.add %71, %2  : i64
    llvm.br ^bb4(%1808 : i64)
  ^bb6:  // pred: ^bb4
    %1809 = llvm.add %65, %63  : i64
    llvm.br ^bb2(%1809 : i64)
  ^bb7:  // pred: ^bb2
    llvm.return
  }
}

